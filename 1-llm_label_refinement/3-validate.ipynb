{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d1c00c-b6c2-4b30-9c0f-22b7e341f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import simple_icd_10_cm as cm\n",
    "import os, json, time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68237212-eed0-4737-93d9-3b9e949698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, filter, and sample (n=10,000) the data\n",
    "def load_sampled_mimic(path=\"processed_data/mimiciv_icd10.parquet\", n=10_000, min_codes=5, max_codes=11):\n",
    "    df = pd.read_parquet(path, columns=[\"text\", \"diagnosis_codes\", \"note_id\"])\n",
    "    df = df[df[\"diagnosis_codes\"].apply(lambda x: isinstance(x, (list, np.ndarray)) and min_codes <= len(x) < max_codes)]\n",
    "    return df.sample(n=min(n, len(df)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22e915f-5f76-45e7-821e-e63f4485247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "\n",
    "def add_note_ids(jsonl=\"processed_data/processed_inputs.jsonl\",\n",
    "                 parquet=\"processed_data/mimiciv_icd10.parquet\",\n",
    "                 out=\"processed_data/processed_inputs_with_ids.jsonl\",\n",
    "                 n=10_000, min_codes=5, max_codes=11):\n",
    "\n",
    "    df = load_sampled_mimic(path=parquet, n=n, min_codes=min_codes, max_codes=max_codes)\n",
    "\n",
    "    data = [json.loads(l) for l in open(jsonl)]\n",
    "    print(len(data))\n",
    "    print(len(df))\n",
    "    assert len(data)==len(df)\n",
    "\n",
    "    for d,(_,r) in zip(data,df.iterrows()):\n",
    "        if len(d[\"discharge_summary\"])!=len(r.text):\n",
    "            raise ValueError(\"Length mismatch\")\n",
    "        d[\"note_id\"]=r.note_id\n",
    "\n",
    "    with open(out,\"w\") as f: f.writelines(json.dumps(d)+\"\\n\" for d in data)\n",
    "    print(f\"Saved {len(data)} with note_id → {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9e7f93-8dfd-41bc-9b70-4c4863032428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "Saved 10000 with note_id → processed_data/processed_inputs_with_ids.jsonl\n"
     ]
    }
   ],
   "source": [
    "add_note_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f264e1e-856c-45de-920a-ccaa60fe8a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combined': PosixPath('processed_data/combined.json'),\n",
       " 'combined_no_ds': PosixPath('processed_data/combined_no_ds.json')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def _load_json_or_jsonl(p: str) -> List[Dict[str, Any]]:\n",
    "    s = Path(p).read_text().strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if s[0] == '[':\n",
    "        return json.loads(s)\n",
    "    return [json.loads(l) for l in s.splitlines() if l.strip()]\n",
    "\n",
    "def _load_json(p: str) -> Dict[str, Any]:\n",
    "    return json.loads(Path(p).read_text())\n",
    "\n",
    "def _has_evidence(e) -> bool:\n",
    "    if e is None:\n",
    "        return False\n",
    "    if isinstance(e, list):\n",
    "        return any(str(x).strip() and str(x).strip().lower() != 'no evidence' for x in e)\n",
    "    s = str(e).strip()\n",
    "    return bool(s) and s.lower() != 'no evidence'\n",
    "\n",
    "def _build_combined(inputs: List[Dict], ckpt: Dict[str, Any]) -> List[Dict]:\n",
    "    out = []\n",
    "    for k, entries in ckpt.items():\n",
    "        try:\n",
    "            i = int(k)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if not (0 <= i < len(inputs)):\n",
    "            continue\n",
    "        note = inputs[i]\n",
    "        mimic_codes = [{'code': c.get('code'), 'title': c.get('title')}\n",
    "                       for c in note.get('icd_gold_standard', [])]\n",
    "        llm_validated = []\n",
    "        llm_accurate = []\n",
    "        for ent in entries:\n",
    "            if not _has_evidence(ent.get('evidence')):\n",
    "                continue\n",
    "            code = ent.get('code')\n",
    "            title = ent.get('title')\n",
    "            accuracy = ent.get('accuracy') or {}\n",
    "            better = accuracy.get('better_alternative') if isinstance(accuracy, dict) else None\n",
    "            llm_validated.append({\n",
    "                'code': code, 'title': title,\n",
    "                'evidence': ent.get('evidence'),\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            code2 = better if (isinstance(better, str) and better.strip()) else code\n",
    "            llm_accurate.append({\n",
    "                'code': code2, 'title': title,\n",
    "                'evidence': ent.get('evidence'),\n",
    "                'replaced_from': code if code2 != code else None\n",
    "            })\n",
    "        out.append({\n",
    "            'index': i,\n",
    "            'note_id': note.get('note_id'),\n",
    "            'discharge_summary': note.get('discharge_summary'),\n",
    "            'mimic_codes': mimic_codes,\n",
    "            'llm_validated_codes': llm_validated,\n",
    "            'llm_accurate_codes': llm_accurate\n",
    "        })\n",
    "    return sorted(out, key=lambda x: x['index'])\n",
    "\n",
    "def process_icd_validation(inputs_path: str,\n",
    "                           ckpt_path: str,\n",
    "                           out_dir: str = 'processed_data') -> Dict[str, Path]:\n",
    "    inputs = _load_json_or_jsonl(inputs_path)\n",
    "    ckpt = _load_json(ckpt_path)\n",
    "    combined = _build_combined(inputs, ckpt)\n",
    "\n",
    "    # Create output directory\n",
    "    outp = Path(out_dir)\n",
    "    outp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save full version\n",
    "    combined_path = outp / 'combined.json'\n",
    "    combined_path.write_text(json.dumps(combined, indent=2))\n",
    "\n",
    "    # Save version with empty discharge summaries\n",
    "    combined_no_ds = [\n",
    "        {**entry, 'discharge_summary': ''} for entry in combined\n",
    "    ]\n",
    "    combined_no_ds_path = outp / 'combined_no_ds.json'\n",
    "    combined_no_ds_path.write_text(json.dumps(combined_no_ds, indent=2))\n",
    "\n",
    "    return {\"combined\": combined_path, \"combined_no_ds\": combined_no_ds_path}\n",
    "\n",
    "# run:\n",
    "process_icd_validation(\n",
    "    'processed_data/processed_inputs_with_ids.jsonl',\n",
    "    'processed_data/icd_verification_ckpt.json',\n",
    "    out_dir='processed_data'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b43e8b-b9db-4bff-9678-795bfe37cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "def describe_groups(data):\n",
    "    defs = {\n",
    "        \"mimic_codes\": \"MIMIC gold standard (original ICD codes in dataset)\",\n",
    "        \"llm_validated_codes\": \"LLM validated codes (only those with evidence kept)\",\n",
    "        \"llm_accurate_codes\": \"LLM accurate codes (validated + better alternatives replacing originals)\"\n",
    "    }\n",
    "    norm = lambda c: c.strip().upper() if isinstance(c, str) else str(c)\n",
    "\n",
    "    print(\"\\n=== Group definitions ===\")\n",
    "    for k,v in defs.items(): print(f\"{k}: {v}\")\n",
    "\n",
    "    stats, sets = {}, {}\n",
    "    for grp in defs:\n",
    "        all_codes = [norm(c.get(\"code\",\"\") if isinstance(c, dict) else c) for note in data for c in note.get(grp,[])]\n",
    "        sets[grp] = set(all_codes)\n",
    "        counts_per_note = [len(note.get(grp,[])) for note in data]\n",
    "        stats[grp] = {\n",
    "            \"notes\": len(data),\n",
    "            \"total\": len(all_codes),\n",
    "            \"avg_per_note\": round(sum(counts_per_note)/len(data),2),\n",
    "            \"median_per_note\": int(statistics.median(counts_per_note)),\n",
    "            \"unique\": len(sets[grp]),\n",
    "            \"top5\": Counter(all_codes).most_common(5)\n",
    "        }\n",
    "\n",
    "    print(\"\\n=== Descriptive stats ===\")\n",
    "    for grp,v in stats.items():\n",
    "        print(f\"\\n{grp}: Notes={v['notes']}, Total={v['total']}, Avg/note={v['avg_per_note']}, Median/note={v['median_per_note']}, Unique={v['unique']}\")\n",
    "        print(\"  Top5:\", \", \".join(f\"{c}({n})\" for c,n in v[\"top5\"]))\n",
    "\n",
    "    print(\"\\n=== Overlap between groups ===\")\n",
    "    keys = list(defs.keys())\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i+1,len(keys)):\n",
    "            a,b = keys[i],keys[j]\n",
    "            inter = sets[a]&sets[b]\n",
    "            print(f\"{a} ∩ {b}: {len(inter)} (≈{len(inter)/len(sets[a])*100:.1f}% of {a}, {len(inter)/len(sets[b])*100:.1f}% of {b})\")\n",
    "\n",
    "    # Evidence / accuracy\n",
    "    total_mimic = sum(len(note.get(\"mimic_codes\",[])) for note in data)\n",
    "    total_validated = sum(len(note.get(\"llm_validated_codes\",[])) for note in data)\n",
    "    codes_lacking_evidence = total_mimic - total_validated\n",
    "\n",
    "    not_accurate = sum(1 for note in data for ent in note.get(\"llm_validated_codes\",[]) if (ent.get(\"accuracy\") or {}).get(\"is_accurate\") is False)\n",
    "\n",
    "    print(\"\\n=== Evidence / Accuracy checks ===\")\n",
    "    print(f\"Total validated entries: {total_validated}\")\n",
    "    print(f\"Codes lacking evidence (MIMIC minus LLM validated): {codes_lacking_evidence} (avg {round(codes_lacking_evidence/len(data),3)} per note)\")\n",
    "    print(f\"Codes with evidence but flagged NOT accurate: {not_accurate} (avg {round(not_accurate/len(data),3)} per note)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a2c6c98-685a-49ff-8bb9-8c984b7ca4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Group definitions ===\n",
      "mimic_codes: MIMIC gold standard (original ICD codes in dataset)\n",
      "llm_validated_codes: LLM validated codes (only those with evidence kept)\n",
      "llm_accurate_codes: LLM accurate codes (validated + better alternatives replacing originals)\n",
      "\n",
      "=== Descriptive stats ===\n",
      "\n",
      "mimic_codes: Notes=9528, Total=73389, Avg/note=7.7, Median/note=8, Unique=4453\n",
      "  Top5: I10(3574), E78.5(2326), Z87.891(1938), K21.9(1651), F32.9(1318)\n",
      "\n",
      "llm_validated_codes: Notes=9528, Total=55638, Avg/note=5.84, Median/note=6, Unique=4075\n",
      "  Top5: I10(3138), E78.5(1982), K21.9(1142), F32.9(953), F41.9(883)\n",
      "\n",
      "llm_accurate_codes: Notes=9528, Total=55638, Avg/note=5.84, Median/note=6, Unique=4685\n",
      "  Top5: I10(3141), E78.5(1922), K21.9(1126), F32.9(871), E11.9(793)\n",
      "\n",
      "=== Overlap between groups ===\n",
      "mimic_codes ∩ llm_validated_codes: 4075 (≈91.5% of mimic_codes, 100.0% of llm_validated_codes)\n",
      "mimic_codes ∩ llm_accurate_codes: 3857 (≈86.6% of mimic_codes, 82.3% of llm_accurate_codes)\n",
      "llm_validated_codes ∩ llm_accurate_codes: 3822 (≈93.8% of llm_validated_codes, 81.6% of llm_accurate_codes)\n",
      "\n",
      "=== Evidence / Accuracy checks ===\n",
      "Total validated entries: 55638\n",
      "Codes lacking evidence (MIMIC minus LLM validated): 17751 (avg 1.863 per note)\n",
      "Codes with evidence but flagged NOT accurate: 6097 (avg 0.64 per note)\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(\"processed_data/combined.json\"))\n",
    "describe_groups(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
