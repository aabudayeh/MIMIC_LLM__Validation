{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcf002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Stage 1/4: Loading runs from /home/aabudaye/medical-coding-reproducibility/files\n",
      "   Loaded LAAT/Original from 8i9s77f0: tuned threshold=0.320000 (took 1.5s)\n",
      "   Loaded LAAT/EV from kqo0vsqc: tuned threshold=0.340000 (took 1.2s)\n",
      "   Loaded LAAT/ER from 5rj0tg1v: tuned threshold=0.310000 (took 1.4s)\n",
      "   Loaded CAML/Original from p9hj5xe1: tuned threshold=0.240000 (took 1.3s)\n",
      "   Loaded CAML/EV from 05cvm0pf: tuned threshold=0.280000 (took 1.2s)\n",
      "   Loaded CAML/ER from b1tl66nj: tuned threshold=0.280000 (took 1.4s)\n",
      "   Loaded MultiResCNN/Original from yz8bpcmd: tuned threshold=0.220000 (took 1.3s)\n",
      "   Loaded MultiResCNN/EV from xxl6md4r: tuned threshold=0.370000 (took 1.2s)\n",
      "   Loaded MultiResCNN/ER from 974v2pcx: tuned threshold=0.330000 (took 1.4s)\n",
      "   Loaded RNN/Original from 9fd18hoi: tuned threshold=0.170000 (took 1.2s)\n",
      "   Loaded RNN/EV from lrjwycyo: tuned threshold=0.210000 (took 1.1s)\n",
      "   Loaded RNN/ER from rmxrgnnu: tuned threshold=0.180000 (took 1.2s)\n",
      "   Loaded CNN/Original from o91zacs0: tuned threshold=0.280000 (took 1.4s)\n",
      "   Loaded CNN/EV from qj24zl5u: tuned threshold=0.340000 (took 1.2s)\n",
      "   Loaded CNN/ER from mlpgnhsf: tuned threshold=0.380000 (took 1.4s)\n",
      "==> Stage 2/4: Planning bootstrap workload\n",
      "   Planned bootstrap iterations: 50000\n",
      "==> Stage 3/4: Running per-model bootstrap CIs\n",
      "[Bootstrapping] 30000/50000 (60.0%) | elapsed 48.2s | ETA 32.1s\n",
      "==> Stage 4/4: Running paired delta bootstraps\n",
      "[Bootstrapping] 50000/50000 (100.0%) | elapsed 112.5s | ETA 0.0s\n",
      "\n",
      "==> Done. Saved results to:\n",
      "   bootstrap_results/per_model_ci.csv\n",
      "   bootstrap_results/paired_deltas_ci.csv\n",
      "   bootstrap_results/summary_across_architectures.csv\n"
     ]
    }
   ],
   "source": [
    "# bootstrap_eval.py\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CONFIG: your run folders\n",
    "# ----------------------------\n",
    "BASE_DIR = Path(\"\")  # <- CHANGE THIS to the parent directory containing the run folders\n",
    "\n",
    "models_labelset_paths = {\n",
    "    \"LAAT\":       {\"Original\": \"8i9s77f0\", \"EV\": \"kqo0vsqc\", \"ER\": \"5rj0tg1v\"},\n",
    "    \"CAML\":       {\"Original\": \"p9hj5xe1\", \"EV\": \"05cvm0pf\", \"ER\": \"b1tl66nj\"},\n",
    "    \"MultiResCNN\":{\"Original\": \"yz8bpcmd\", \"EV\": \"xxl6md4r\", \"ER\": \"974v2pcx\"},\n",
    "    \"RNN\":        {\"Original\": \"9fd18hoi\", \"EV\": \"lrjwycyo\", \"ER\": \"rmxrgnnu\"},\n",
    "    \"CNN\":        {\"Original\": \"o91zacs0\", \"EV\": \"qj24zl5u\", \"ER\": \"mlpgnhsf\"},\n",
    "    # add others here if needed\n",
    "}\n",
    "\n",
    "# Choose which label set to use as the EVALUATION reference for the test set:\n",
    "EVAL_REFERENCE = \"EV\"        # options: \"EV\", \"Original\", \"ER\"\n",
    "N_BOOT = 2000                      # bootstrap replicates\n",
    "RNG = np.random.default_rng(13)\n",
    "\n",
    "# How often to update the console progress line (in iterations)\n",
    "PROGRESS_UPDATE_EVERY = 200\n",
    "\n",
    "# Choose which label set to use as the EVALUATION reference for the test set:\n",
    "EVAL_REFERENCE = \"EV\"  # options: \"EV\", \"Original\", \"ER\"\n",
    "\n",
    "# Threshold tuning mode (for validation-based tuning)\n",
    "#   \"diagnostic\" -> np.linspace(0.01, 0.99, 99)  (matches your diagnostic script)\n",
    "#   \"wide\"       -> includes low thresholds (1e-6..0.5) for tiny scores\n",
    "THRESHOLD_MODE = \"diagnostic\"   # or \"wide\"\n",
    "\n",
    "if THRESHOLD_MODE == \"diagnostic\":\n",
    "    THRESHOLD_GRID = np.linspace(0.01, 0.99, 99)\n",
    "elif THRESHOLD_MODE == \"wide\":\n",
    "    THRESHOLD_GRID = np.unique(np.concatenate([\n",
    "        np.array([1e-6, 5e-6, 1e-5, 5e-5]),\n",
    "        np.logspace(-4, -1, 13),   # 0.0001 → 0.1 (log-spaced)\n",
    "        np.linspace(0.1, 0.5, 9),  # 0.1 → 0.5\n",
    "    ]))\n",
    "else:\n",
    "    raise ValueError(\"THRESHOLD_MODE must be 'diagnostic' or 'wide'\")\n",
    "\n",
    "N_BOOT = 2000\n",
    "RNG = np.random.default_rng(13)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Tiny progress helper\n",
    "# ----------------------------\n",
    "class Progress:\n",
    "    def __init__(self, total: int, label: str = \"Progress\", update_every: int = PROGRESS_UPDATE_EVERY):\n",
    "        self.total = int(total)\n",
    "        self.label = label\n",
    "        self.update_every = max(1, int(update_every))\n",
    "        self.start = time.time()\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, n: int = 1):\n",
    "        self.count += n\n",
    "        if (self.count == 1) or (self.count % self.update_every == 0) or (self.count >= self.total):\n",
    "            elapsed = time.time() - self.start\n",
    "            rate = self.count / elapsed if elapsed > 0 else 0.0\n",
    "            remaining = (self.total - self.count) / rate if rate > 0 else float(\"inf\")\n",
    "            pct = (self.count / self.total * 100.0) if self.total > 0 else 100.0\n",
    "            msg = f\"\\r[{self.label}] {self.count}/{self.total} ({pct:.1f}%) | elapsed {elapsed:.1f}s | ETA {remaining:.1f}s\"\n",
    "            sys.stdout.write(msg)\n",
    "            sys.stdout.flush()\n",
    "            if self.count >= self.total:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Helpers\n",
    "# ----------------------------\n",
    "def ensure_list(x):\n",
    "    import numpy as np\n",
    "    import ast\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.astype(str).tolist()\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(x)\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return [str(v) for v in val]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "def _read_feather(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    return pd.read_feather(p)\n",
    "\n",
    "def _code_columns(df: pd.DataFrame) -> List[str]:\n",
    "    # assumes non-code columns are exactly {'_id','target'}\n",
    "    return [c for c in df.columns if c not in (\"_id\", \"target\")]\n",
    "\n",
    "def _targets_dict(df: pd.DataFrame) -> Dict[str, Set[str]]:\n",
    "    out = {}\n",
    "    for _, row in df[[\"_id\", \"target\"]].iterrows():\n",
    "        nid = str(row[\"_id\"])\n",
    "        codes = set(row[\"target\"]) if isinstance(row[\"target\"], (list, tuple, set)) else set()\n",
    "        out[nid] = codes\n",
    "    return out\n",
    "\n",
    "def _as_prob_matrix(df: pd.DataFrame, codes: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return an (N x C) matrix of probabilities in [0,1].\n",
    "    If values look like logits (outside [0,1]), apply sigmoid.\n",
    "    \"\"\"\n",
    "    X = df[codes].to_numpy(dtype=float)\n",
    "    xmin, xmax = np.nanmin(X), np.nanmax(X)\n",
    "    if (xmin < -1e-6) or (xmax > 1 + 1e-6):\n",
    "        X = 1.0 / (1.0 + np.exp(-X))  # sigmoid\n",
    "    return X\n",
    "\n",
    "def _pred_sets_from_df(df: pd.DataFrame, threshold: float) -> Dict[str, Set[str]]:\n",
    "    codes = _code_columns(df)\n",
    "    code_arr = np.array(codes, dtype=object)\n",
    "    X = _as_prob_matrix(df, codes)\n",
    "    meets = X >= threshold  # boolean mask\n",
    "    id2pred = {}\n",
    "    ids = df[\"_id\"].astype(str).tolist()\n",
    "    for i, nid in enumerate(ids):\n",
    "        id2pred[nid] = set(code_arr[meets[i]])\n",
    "    return id2pred\n",
    "\n",
    "def _micro_counts(pred_sets: Dict[str, Set[str]],\n",
    "                  true_sets: Dict[str, Set[str]],\n",
    "                  ids: List[str]) -> Tuple[int,int,int]:\n",
    "    TP = FP = FN = 0\n",
    "    for nid in ids:\n",
    "        p = pred_sets.get(nid, set())\n",
    "        t = true_sets.get(nid, set())\n",
    "        TP += len(p & t)\n",
    "        FP += len(p - t)\n",
    "        FN += len(t - p)\n",
    "    return TP, FP, FN\n",
    "\n",
    "def _micro_metrics_from_counts(TP:int, FP:int, FN:int) -> Tuple[float,float,float]:\n",
    "    prec = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    rec  = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def _sweep_threshold_for_val(val_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Pick a single global threshold that maximizes micro-F1 on the given val file.\n",
    "    Grid is controlled by THRESHOLD_GRID, set from THRESHOLD_MODE at top.\n",
    "    \"\"\"\n",
    "    codes = _code_columns(val_df)\n",
    "    X = _as_prob_matrix(val_df, codes)\n",
    "    true_sets = _targets_dict(val_df)\n",
    "    ids = val_df[\"_id\"].astype(str).tolist()\n",
    "\n",
    "    best_t, best_f1 = float(THRESHOLD_GRID[0]), -1.0\n",
    "    code_arr = np.array(codes, dtype=object)\n",
    "    for t in THRESHOLD_GRID:\n",
    "        meets = X >= t\n",
    "        TP = FP = FN = 0\n",
    "        for i, nid in enumerate(ids):\n",
    "            p = set(code_arr[meets[i]])\n",
    "            tset = true_sets.get(nid, set())\n",
    "            TP += len(p & tset); FP += len(p - tset); FN += len(tset - p)\n",
    "        _, _, f1 = _micro_metrics_from_counts(TP, FP, FN)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, float(t)\n",
    "    return best_t\n",
    "\n",
    "\n",
    "# NOTE: we’ll pass a callback into the bootstrap loops so we can update a single global progress bar.\n",
    "def _bootstrap_ci_single(pred_sets: Dict[str, Set[str]],\n",
    "                         true_sets: Dict[str, Set[str]],\n",
    "                         ids: List[str],\n",
    "                         B: int,\n",
    "                         progress_cb=None) -> Dict[str, Tuple[float,float,float,float]]:\n",
    "    \"\"\"Returns dict(metric -> (point, lo, hi, std)) for precision, recall, microF1.\"\"\"\n",
    "    # point estimate on full sample\n",
    "    TP, FP, FN = _micro_counts(pred_sets, true_sets, ids)\n",
    "    p0, r0, f0 = _micro_metrics_from_counts(TP, FP, FN)\n",
    "\n",
    "    # bootstrap\n",
    "    n = len(ids)\n",
    "    p_arr, r_arr, f_arr = np.empty(B), np.empty(B), np.empty(B)\n",
    "    ids_np = np.array(ids, dtype=object)\n",
    "    for b in range(B):\n",
    "        idx = RNG.integers(0, n, size=n)  # sample notes with replacement\n",
    "        boot_ids = ids_np[idx].tolist()\n",
    "        TP, FP, FN = _micro_counts(pred_sets, true_sets, boot_ids)\n",
    "        p, r, f = _micro_metrics_from_counts(TP, FP, FN)\n",
    "        p_arr[b], r_arr[b], f_arr[b] = p, r, f\n",
    "        if progress_cb:\n",
    "            progress_cb(1)\n",
    "\n",
    "    def _ci(a: np.ndarray):\n",
    "        lo, hi = np.percentile(a, [2.5, 97.5])\n",
    "        return lo, hi, float(a.std(ddof=1))\n",
    "    p_lo, p_hi, p_sd = _ci(p_arr)\n",
    "    r_lo, r_hi, r_sd = _ci(r_arr)\n",
    "    f_lo, f_hi, f_sd = _ci(f_arr)\n",
    "    return {\n",
    "        \"precision\": (p0, p_lo, p_hi, p_sd),\n",
    "        \"recall\":    (r0, r_lo, r_hi, r_sd),\n",
    "        \"f1_micro\":  (f0, f_lo, f_hi, f_sd),\n",
    "    }\n",
    "\n",
    "def _paired_bootstrap_delta(predA: Dict[str, Set[str]],\n",
    "                            predB: Dict[str, Set[str]],\n",
    "                            true_sets: Dict[str, Set[str]],\n",
    "                            ids: List[str],\n",
    "                            B: int,\n",
    "                            progress_cb=None) -> Dict[str, Tuple[float,float,float,float]]:\n",
    "    \"\"\"CI for delta metrics: A - B (paired over same bootstrapped note ids).\n",
    "       Returns deltas and one-sided p-values for Precision, Recall, and F1 (test A > B).\"\"\"\n",
    "    # point deltas (not used for p, but useful to keep consistent with CI)\n",
    "    TP_A, FP_A, FN_A = _micro_counts(predA, true_sets, ids)\n",
    "    TP_B, FP_B, FN_B = _micro_counts(predB, true_sets, ids)\n",
    "    pA, rA, fA = _micro_metrics_from_counts(TP_A, FP_A, FN_A)\n",
    "    pB, rB, fB = _micro_metrics_from_counts(TP_B, FP_B, FN_B)\n",
    "\n",
    "    # paired bootstrap\n",
    "    n = len(ids)\n",
    "    ids_np = np.array(ids, dtype=object)\n",
    "    dp, dr, df = np.empty(B), np.empty(B), np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = RNG.integers(0, n, size=n)\n",
    "        boot_ids = ids_np[idx].tolist()\n",
    "        TP_A, FP_A, FN_A = _micro_counts(predA, true_sets, boot_ids)\n",
    "        TP_B, FP_B, FN_B = _micro_counts(predB, true_sets, boot_ids)\n",
    "        pA, rA, fA = _micro_metrics_from_counts(TP_A, FP_A, FN_A)\n",
    "        pB, rB, fB = _micro_metrics_from_counts(TP_B, FP_B, FN_B)\n",
    "        dp[b], dr[b], df[b] = (pA - pB), (rA - rB), (fA - fB)\n",
    "        if progress_cb:\n",
    "            progress_cb(1)\n",
    "\n",
    "    def _ci(a: np.ndarray):\n",
    "        lo, hi = np.percentile(a, [2.5, 97.5])\n",
    "        return float(a.mean()), float(lo), float(hi), float(a.std(ddof=1))\n",
    "\n",
    "    # One-sided p-values for the hypothesis A > B\n",
    "    p_one_sided_precision = float((dp <= 0).mean())\n",
    "    p_one_sided_recall    = float((dr <= 0).mean())\n",
    "    p_one_sided_f1        = float((df <= 0).mean())\n",
    "\n",
    "    return {\n",
    "        \"d_precision\": _ci(dp),\n",
    "        \"d_recall\":    _ci(dr),\n",
    "        \"d_f1_micro\":  _ci(df),\n",
    "        \"p_one_sided_precision\": p_one_sided_precision,\n",
    "        \"p_one_sided_recall\":    p_one_sided_recall,\n",
    "        \"p_one_sided_f1\":        p_one_sided_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Main evaluation routine\n",
    "# ----------------------------\n",
    "def load_run(folder: Path):\n",
    "    \"\"\"Return (val_df, test_df) for a single run.\"\"\"\n",
    "    val_df  = _read_feather(folder / \"predictions_val.feather\")\n",
    "    test_df = _read_feather(folder / \"predictions_test.feather\")\n",
    "\n",
    "    # normalize targets to lists (handles numpy arrays or strings)\n",
    "    val_df[\"target\"]  = val_df[\"target\"].apply(ensure_list)\n",
    "    test_df[\"target\"] = test_df[\"target\"].apply(ensure_list)\n",
    "\n",
    "    return val_df, test_df\n",
    "\n",
    "def main():\n",
    "    print(f\"==> Stage 1/4: Loading runs from {BASE_DIR.resolve()}\")\n",
    "    per_arch = {}  # arch -> bundle\n",
    "    total_runs = 0\n",
    "\n",
    "    # Load + threshold tuning + prepare predictions/labels\n",
    "    for arch, paths in models_labelset_paths.items():\n",
    "        loaded = {}\n",
    "        for labelset, folder_name in paths.items():\n",
    "            folder = BASE_DIR / folder_name\n",
    "            if not folder.exists():\n",
    "                print(f\"   [WARN] Missing folder: {folder}\")\n",
    "                continue\n",
    "            total_runs += 1\n",
    "            val_df, test_df = load_run(folder)\n",
    "\n",
    "            # threshold tuned on this run's validation\n",
    "            t0 = time.time()\n",
    "            t = _sweep_threshold_for_val(val_df)\n",
    "            print(f\"   Loaded {arch}/{labelset} from {folder.name}: tuned threshold={t:.6f} (took {time.time()-t0:.1f}s)\")\n",
    "\n",
    "            # predicted sets for test at tuned threshold\n",
    "            pred_sets = _pred_sets_from_df(test_df, t)\n",
    "            # run's own ground-truth (used to pick reference later)\n",
    "            true_sets = _targets_dict(test_df)\n",
    "            ids = [str(x) for x in test_df[\"_id\"].tolist()]\n",
    "\n",
    "            loaded[labelset] = dict(\n",
    "                folder=str(folder),\n",
    "                threshold=t,\n",
    "                pred_sets=pred_sets,\n",
    "                true_sets=true_sets,\n",
    "                ids=ids,\n",
    "            )\n",
    "\n",
    "        if not loaded:\n",
    "            continue\n",
    "\n",
    "        # reference labels inside this architecture:\n",
    "        if EVAL_REFERENCE not in loaded:\n",
    "            # fallback if requested reference missing\n",
    "            fallback = next(iter(loaded.keys()))\n",
    "            print(f\"   [WARN] {arch}: requested EVAL_REFERENCE={EVAL_REFERENCE} not found. Falling back to {fallback}.\")\n",
    "            ref_true = loaded[fallback][\"true_sets\"]\n",
    "        else:\n",
    "            ref_true = loaded[EVAL_REFERENCE][\"true_sets\"]\n",
    "\n",
    "        # intersect note ids across available labelsets for this arch\n",
    "        common_ids = None\n",
    "        for k in loaded:\n",
    "            s = set(loaded[k][\"ids\"])\n",
    "            common_ids = s if common_ids is None else (common_ids & s)\n",
    "        common_ids = sorted(list(common_ids)) if common_ids else []\n",
    "\n",
    "        per_arch[arch] = {\n",
    "            \"loaded\": loaded,\n",
    "            \"ref_true_sets\": ref_true,\n",
    "            \"common_ids\": common_ids,\n",
    "        }\n",
    "\n",
    "    if not per_arch:\n",
    "        print(\"No runs loaded. Check BASE_DIR and folder names.\")\n",
    "        return\n",
    "\n",
    "    # Compute total bootstrap iterations for progress/ETA\n",
    "    print(\"==> Stage 2/4: Planning bootstrap workload\")\n",
    "    total_boot_iters = 0\n",
    "    for arch, bundle in per_arch.items():\n",
    "        ids = bundle[\"common_ids\"]\n",
    "        if not ids:\n",
    "            continue\n",
    "        runs = bundle[\"loaded\"]\n",
    "        # per-model CIs (one per available labelset)\n",
    "        total_boot_iters += len(runs) * N_BOOT\n",
    "        # paired deltas (EV-Orig, ER-Orig) if present\n",
    "        if \"EV\" in runs and \"Original\" in runs:\n",
    "            total_boot_iters += N_BOOT\n",
    "        if \"ER\" in runs and \"Original\" in runs:\n",
    "            total_boot_iters += N_BOOT\n",
    "    print(f\"   Planned bootstrap iterations: {total_boot_iters}\")\n",
    "\n",
    "    progress = Progress(total=total_boot_iters, label=\"Bootstrapping\", update_every=PROGRESS_UPDATE_EVERY)\n",
    "    progress_cb = progress.update if total_boot_iters > 0 else None\n",
    "\n",
    "    # 3a) Per-model (descriptive) CIs vs chosen reference labels\n",
    "    print(\"==> Stage 3/4: Running per-model bootstrap CIs\")\n",
    "    per_model_records = []\n",
    "    for arch, bundle in per_arch.items():\n",
    "        ref_true = bundle[\"ref_true_sets\"]\n",
    "        ids = bundle[\"common_ids\"]\n",
    "        if not ids:\n",
    "            print(f\"   [INFO] {arch}: no common IDs across labelsets; skipping.\")\n",
    "            continue\n",
    "        for labelset, run in bundle[\"loaded\"].items():\n",
    "            ci = _bootstrap_ci_single(run[\"pred_sets\"], ref_true, ids, B=N_BOOT, progress_cb=progress_cb)\n",
    "            per_model_records.append({\n",
    "                \"architecture\": arch,\n",
    "                \"trained_on\": labelset,\n",
    "                \"folder\": run[\"folder\"],\n",
    "                \"threshold\": run[\"threshold\"],\n",
    "                \"precision\": ci[\"precision\"][0],\n",
    "                \"precision_lo\": ci[\"precision\"][1],\n",
    "                \"precision_hi\": ci[\"precision\"][2],\n",
    "                \"recall\": ci[\"recall\"][0],\n",
    "                \"recall_lo\": ci[\"recall\"][1],\n",
    "                \"recall_hi\": ci[\"recall\"][2],\n",
    "                \"f1_micro\": ci[\"f1_micro\"][0],\n",
    "                \"f1_lo\": ci[\"f1_micro\"][1],\n",
    "                \"f1_hi\": ci[\"f1_micro\"][2],\n",
    "            })\n",
    "    per_model_df = pd.DataFrame(per_model_records).sort_values([\"architecture\", \"trained_on\"])\n",
    "\n",
    "    # 3b) Paired bootstrap deltas inside each architecture (EV-Orig, ER-Orig)\n",
    "    print(\"\\n==> Stage 4/4: Running paired delta bootstraps\")\n",
    "    delta_records = []\n",
    "    for arch, bundle in per_arch.items():\n",
    "        ref_true = bundle[\"ref_true_sets\"]\n",
    "        ids = bundle[\"common_ids\"]\n",
    "        if not ids:\n",
    "            continue\n",
    "        runs = bundle[\"loaded\"]\n",
    "        # EV vs Original\n",
    "        if \"EV\" in runs and \"Original\" in runs:\n",
    "            d = _paired_bootstrap_delta(runs[\"EV\"][\"pred_sets\"], runs[\"Original\"][\"pred_sets\"], ref_true, ids, B=N_BOOT, progress_cb=progress_cb)\n",
    "            delta_records.append({\n",
    "                \"architecture\": arch, \"comparison\": \"EV - Original\",\n",
    "                \"d_precision\": d[\"d_precision\"][0], \"d_precision_lo\": d[\"d_precision\"][1], \"d_precision_hi\": d[\"d_precision\"][2],\n",
    "                \"d_recall\":    d[\"d_recall\"][0],    \"d_recall_lo\":    d[\"d_recall\"][1],    \"d_recall_hi\":    d[\"d_recall\"][2],\n",
    "                \"d_f1\":        d[\"d_f1_micro\"][0],  \"d_f1_lo\":        d[\"d_f1_micro\"][1],  \"d_f1_hi\":        d[\"d_f1_micro\"][2],\n",
    "                \"p_one_sided_precision\": d[\"p_one_sided_precision\"],\n",
    "                \"p_one_sided_recall\":    d[\"p_one_sided_recall\"],\n",
    "                \"p_one_sided_f1\":        d[\"p_one_sided_f1\"],\n",
    "            })\n",
    "\n",
    "        # ER vs Original\n",
    "        if \"ER\" in runs and \"Original\" in runs:\n",
    "            d = _paired_bootstrap_delta(runs[\"ER\"][\"pred_sets\"], runs[\"Original\"][\"pred_sets\"], ref_true, ids, B=N_BOOT, progress_cb=progress_cb)\n",
    "            delta_records.append({\n",
    "                \"architecture\": arch, \"comparison\": \"ER - Original\",\n",
    "                \"d_precision\": d[\"d_precision\"][0], \"d_precision_lo\": d[\"d_precision\"][1], \"d_precision_hi\": d[\"d_precision\"][2],\n",
    "                \"d_recall\":    d[\"d_recall\"][0],    \"d_recall_lo\":    d[\"d_recall\"][1],    \"d_recall_hi\":    d[\"d_recall\"][2],\n",
    "                \"d_f1\":        d[\"d_f1_micro\"][0],  \"d_f1_lo\":        d[\"d_f1_micro\"][1],  \"d_f1_hi\":        d[\"d_f1_micro\"][2],\n",
    "                \"p_one_sided_precision\": d[\"p_one_sided_precision\"],\n",
    "                \"p_one_sided_recall\":    d[\"p_one_sided_recall\"],\n",
    "                \"p_one_sided_f1\":        d[\"p_one_sided_f1\"],\n",
    "            })\n",
    "    delta_df = pd.DataFrame(delta_records).sort_values([\"architecture\", \"comparison\"])\n",
    "\n",
    "    # Summary across architectures\n",
    "    if not delta_df.empty:\n",
    "        delta_df[\"sig_f1\"] = (delta_df[\"d_f1_lo\"] > 0).astype(int)\n",
    "        by_comp = delta_df.groupby(\"comparison\")[\"sig_f1\"].agg([\"sum\", \"count\"]).reset_index()\n",
    "        by_comp[\"proportion_sig\"] = by_comp[\"sum\"] / by_comp[\"count\"]\n",
    "    else:\n",
    "        by_comp = pd.DataFrame(columns=[\"comparison\", \"sum\", \"count\", \"proportion_sig\"])\n",
    "\n",
    "    # Save results\n",
    "    out_dir = BASE_DIR / \"bootstrap_results\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    per_model_df.to_csv(out_dir / \"per_model_ci.csv\", index=False)\n",
    "    delta_df.to_csv(out_dir / \"paired_deltas_ci.csv\", index=False)\n",
    "    by_comp.to_csv(out_dir / \"summary_across_architectures.csv\", index=False)\n",
    "\n",
    "    print(\"\\n==> Done. Saved results to:\")\n",
    "    print(f\"   {out_dir / 'per_model_ci.csv'}\")\n",
    "    print(f\"   {out_dir / 'paired_deltas_ci.csv'}\")\n",
    "    print(f\"   {out_dir / 'summary_across_architectures.csv'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc5419d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  architecture trained_on    folder  threshold  precision  precision_lo  \\\n",
      "0         CAML         ER  b1tl66nj       0.28   0.476531      0.463993   \n",
      "1         CAML         EV  05cvm0pf       0.28   0.492162      0.479733   \n",
      "2         CAML   Original  p9hj5xe1       0.24   0.388476      0.378122   \n",
      "3          CNN         ER  mlpgnhsf       0.38   0.254274      0.189174   \n",
      "4          CNN         EV  qj24zl5u       0.34   0.271605      0.200683   \n",
      "\n",
      "   precision_hi    recall  recall_lo  recall_hi  f1_micro     f1_lo     f1_hi  \n",
      "0      0.489653  0.369754   0.358417   0.380855  0.416406  0.406211  0.427178  \n",
      "1      0.505628  0.393753   0.382122   0.405519  0.437492  0.426606  0.448394  \n",
      "2      0.399540  0.428485   0.416919   0.440664  0.407501  0.397785  0.417816  \n",
      "3      0.333080  0.200917   0.190920   0.210669  0.224468  0.194253  0.252738  \n",
      "4      0.353367  0.228172   0.217953   0.237821  0.248001  0.213074  0.278574  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <th>trained_on</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">CAML</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.477</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.492</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.388</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">CNN</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.254</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.272</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.188</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LAAT</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.516</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.512</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.420</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MultiResCNN</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.536</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.529</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">RNN</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.358</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.381</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.269</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision  recall  f1_micro\n",
       "architecture trained_on                             \n",
       "CAML         ER              0.477   0.370     0.416\n",
       "             EV              0.492   0.394     0.437\n",
       "             Original        0.388   0.428     0.408\n",
       "CNN          ER              0.254   0.201     0.224\n",
       "             EV              0.272   0.228     0.248\n",
       "             Original        0.188   0.274     0.223\n",
       "LAAT         ER              0.516   0.403     0.453\n",
       "             EV              0.512   0.378     0.435\n",
       "             Original        0.420   0.417     0.419\n",
       "MultiResCNN  ER              0.536   0.310     0.393\n",
       "             EV              0.529   0.362     0.429\n",
       "             Original        0.367   0.371     0.369\n",
       "RNN          ER              0.358   0.231     0.281\n",
       "             EV              0.381   0.251     0.303\n",
       "             Original        0.269   0.278     0.274"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "per_model = pd.read_csv(\"bootstrap_results/per_model_ci.csv\")\n",
    "print(per_model.head())\n",
    "\n",
    "# Quick summary table by architecture\n",
    "summary = (\n",
    "    per_model.groupby([\"architecture\",\"trained_on\"])\n",
    "    [[\"precision\",\"recall\",\"f1_micro\"]].mean()\n",
    "    .round(3)\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c87202db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Stage 1/4: Loading & tuning (self-referenced)\n",
      "   Loaded LAAT/Original from 8i9s77f0: tuned threshold=0.320000 (took 1.5s)\n",
      "   Loaded LAAT/EV from kqo0vsqc: tuned threshold=0.340000 (took 1.3s)\n",
      "   Loaded LAAT/ER from 5rj0tg1v: tuned threshold=0.310000 (took 1.5s)\n",
      "   Loaded CAML/Original from p9hj5xe1: tuned threshold=0.240000 (took 1.4s)\n",
      "   Loaded CAML/EV from 05cvm0pf: tuned threshold=0.280000 (took 1.2s)\n",
      "   Loaded CAML/ER from b1tl66nj: tuned threshold=0.280000 (took 1.4s)\n",
      "   Loaded MultiResCNN/Original from yz8bpcmd: tuned threshold=0.220000 (took 1.3s)\n",
      "   Loaded MultiResCNN/EV from xxl6md4r: tuned threshold=0.370000 (took 1.2s)\n",
      "   Loaded MultiResCNN/ER from 974v2pcx: tuned threshold=0.330000 (took 1.3s)\n",
      "   Loaded RNN/Original from 9fd18hoi: tuned threshold=0.170000 (took 1.3s)\n",
      "   Loaded RNN/EV from lrjwycyo: tuned threshold=0.210000 (took 1.2s)\n",
      "   Loaded RNN/ER from rmxrgnnu: tuned threshold=0.180000 (took 1.3s)\n",
      "   Loaded CNN/Original from o91zacs0: tuned threshold=0.280000 (took 1.4s)\n",
      "   Loaded CNN/EV from qj24zl5u: tuned threshold=0.340000 (took 1.2s)\n",
      "   Loaded CNN/ER from mlpgnhsf: tuned threshold=0.380000 (took 1.4s)\n",
      "==> Stage 2/4: Planning bootstrap workload\n",
      "   Planned bootstrap iterations: 60000\n",
      "==> Stage 3/4: Per-model self-referenced CIs\n",
      "[Bootstrapping] 30000/60000 (50.0%) | elapsed 53.4s | ETA 53.4s\n",
      "==> Stage 4/4: Paired deltas (self-referenced on each side)\n",
      "[Bootstrapping] 60000/60000 (100.0%) | elapsed 158.6s | ETA 0.0s\n",
      "\n",
      "==> Done. Saved results to:\n",
      "   bootstrap_results_selfref/per_model_ci.csv\n",
      "   bootstrap_results_selfref/paired_deltas_ci.csv\n"
     ]
    }
   ],
   "source": [
    "# bootstrap_self_eval.py\n",
    "import json, time, sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# 1) CONFIG\n",
    "# ============================\n",
    "BASE_DIR = Path(\"\")  # <- set to parent folder that contains run folders (e.g., \"8i9s77f0\", ...)\n",
    "models_labelset_paths = {\n",
    "    \"LAAT\":       {\"Original\": \"8i9s77f0\", \"EV\": \"kqo0vsqc\", \"ER\": \"5rj0tg1v\"},\n",
    "    \"CAML\":       {\"Original\": \"p9hj5xe1\", \"EV\": \"05cvm0pf\", \"ER\": \"b1tl66nj\"},\n",
    "    \"MultiResCNN\":{\"Original\": \"yz8bpcmd\", \"EV\": \"xxl6md4r\", \"ER\": \"974v2pcx\"},\n",
    "    \"RNN\":        {\"Original\": \"9fd18hoi\", \"EV\": \"lrjwycyo\", \"ER\": \"rmxrgnnu\"},\n",
    "    \"CNN\":        {\"Original\": \"o91zacs0\", \"EV\": \"qj24zl5u\", \"ER\": \"mlpgnhsf\"},\n",
    "}\n",
    "\n",
    "# Threshold tuning grid (match your diagnostic script or use the wide one if you ever need it)\n",
    "THRESHOLD_MODE = \"diagnostic\"  # \"diagnostic\" -> np.linspace(0.01, 0.99, 99); \"wide\" -> includes very low thresholds\n",
    "if THRESHOLD_MODE == \"diagnostic\":\n",
    "    THRESHOLD_GRID = np.linspace(0.01, 0.99, 99)\n",
    "elif THRESHOLD_MODE == \"wide\":\n",
    "    THRESHOLD_GRID = np.unique(np.concatenate([\n",
    "        np.array([1e-6, 5e-6, 1e-5, 5e-5]),\n",
    "        np.logspace(-4, -1, 13),\n",
    "        np.linspace(0.1, 0.5, 9),\n",
    "    ]))\n",
    "else:\n",
    "    raise ValueError(\"THRESHOLD_MODE must be 'diagnostic' or 'wide'\")\n",
    "\n",
    "N_BOOT = 2000\n",
    "RNG = np.random.default_rng(13)\n",
    "PROGRESS_UPDATE_EVERY = 200\n",
    "\n",
    "# ============================\n",
    "# 2) Progress helper\n",
    "# ============================\n",
    "class Progress:\n",
    "    def __init__(self, total: int, label: str = \"Progress\", update_every: int = PROGRESS_UPDATE_EVERY):\n",
    "        self.total = int(total)\n",
    "        self.label = label\n",
    "        self.update_every = max(1, int(update_every))\n",
    "        self.start = time.time()\n",
    "        self.count = 0\n",
    "    def update(self, n: int = 1):\n",
    "        self.count += n\n",
    "        if (self.count == 1) or (self.count % self.update_every == 0) or (self.count >= self.total):\n",
    "            elapsed = time.time() - self.start\n",
    "            rate = self.count / elapsed if elapsed > 0 else 0.0\n",
    "            remaining = (self.total - self.count) / rate if rate > 0 else float(\"inf\")\n",
    "            pct = (self.count / self.total * 100.0) if self.total > 0 else 100.0\n",
    "            msg = f\"\\r[{self.label}] {self.count}/{self.total} ({pct:.1f}%) | elapsed {elapsed:.1f}s | ETA {remaining:.1f}s\"\n",
    "            sys.stdout.write(msg); sys.stdout.flush()\n",
    "            if self.count >= self.total: sys.stdout.write(\"\\n\")\n",
    "\n",
    "# ============================\n",
    "# 3) Helpers\n",
    "# ============================\n",
    "def ensure_list(x):\n",
    "    import numpy as np, ast\n",
    "    if x is None: return []\n",
    "    if isinstance(x, (list, tuple)): return list(x)\n",
    "    if isinstance(x, np.ndarray): return x.astype(str).tolist()\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = ast.literal_eval(x)\n",
    "            if isinstance(v, (list, tuple)): return [str(i) for i in v]\n",
    "        except Exception: pass\n",
    "    return []\n",
    "\n",
    "def _read_feather(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists(): raise FileNotFoundError(p)\n",
    "    return pd.read_feather(p)\n",
    "\n",
    "def _code_columns(df: pd.DataFrame) -> List[str]:\n",
    "    return [c for c in df.columns if c not in (\"_id\", \"target\")]\n",
    "\n",
    "def _targets_dict(df: pd.DataFrame) -> Dict[str, Set[str]]:\n",
    "    out = {}\n",
    "    for _, row in df[[\"_id\",\"target\"]].iterrows():\n",
    "        out[str(row[\"_id\"])] = set(row[\"target\"]) if isinstance(row[\"target\"], (list, tuple, set)) else set()\n",
    "    return out\n",
    "\n",
    "def _as_prob_matrix(df: pd.DataFrame, codes: List[str]) -> np.ndarray:\n",
    "    X = df[codes].to_numpy(dtype=float)\n",
    "    xmin, xmax = np.nanmin(X), np.nanmax(X)\n",
    "    if (xmin < -1e-6) or (xmax > 1 + 1e-6):\n",
    "        X = 1.0 / (1.0 + np.exp(-X))  # sigmoid only if outside [0,1]\n",
    "    return X\n",
    "\n",
    "def _pred_sets_from_df(df: pd.DataFrame, threshold: float) -> Dict[str, Set[str]]:\n",
    "    codes = _code_columns(df)\n",
    "    code_arr = np.array(codes, dtype=object)\n",
    "    X = _as_prob_matrix(df, codes)\n",
    "    meets = X >= threshold\n",
    "    ids = df[\"_id\"].astype(str).tolist()\n",
    "    return {nid: set(code_arr[meets[i]]) for i, nid in enumerate(ids)}\n",
    "\n",
    "def _micro_counts(pred_sets: Dict[str, Set[str]],\n",
    "                  true_sets: Dict[str, Set[str]],\n",
    "                  ids: List[str]) -> Tuple[int,int,int]:\n",
    "    TP=FP=FN=0\n",
    "    for nid in ids:\n",
    "        p = pred_sets.get(nid, set()); t = true_sets.get(nid, set())\n",
    "        TP += len(p & t); FP += len(p - t); FN += len(t - p)\n",
    "    return TP, FP, FN\n",
    "\n",
    "def _micro_metrics_from_counts(TP:int, FP:int, FN:int) -> Tuple[float,float,float]:\n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def _sweep_threshold_for_val(val_df: pd.DataFrame) -> float:\n",
    "    codes = _code_columns(val_df)\n",
    "    X = _as_prob_matrix(val_df, codes)\n",
    "    true_sets = _targets_dict(val_df)\n",
    "    ids = val_df[\"_id\"].astype(str).tolist()\n",
    "    best_t, best_f1 = float(THRESHOLD_GRID[0]), -1.0\n",
    "    code_arr = np.array(codes, dtype=object)\n",
    "    for t in THRESHOLD_GRID:\n",
    "        meets = X >= t\n",
    "        TP=FP=FN=0\n",
    "        for i, nid in enumerate(ids):\n",
    "            p = set(code_arr[meets[i]]); tset = true_sets.get(nid, set())\n",
    "            TP += len(p & tset); FP += len(p - tset); FN += len(tset - p)\n",
    "        _,_,f1 = _micro_metrics_from_counts(TP,FP,FN)\n",
    "        if f1 > best_f1: best_f1, best_t = f1, float(t)\n",
    "    return best_t\n",
    "\n",
    "def _bootstrap_ci_single(pred_sets: Dict[str, Set[str]],\n",
    "                         true_sets: Dict[str, Set[str]],\n",
    "                         ids: List[str],\n",
    "                         B: int,\n",
    "                         progress_cb=None) -> Dict[str, Tuple[float,float,float,float]]:\n",
    "    TP,FP,FN = _micro_counts(pred_sets, true_sets, ids)\n",
    "    p0,r0,f0 = _micro_metrics_from_counts(TP,FP,FN)\n",
    "    n = len(ids); ids_np = np.array(ids, dtype=object)\n",
    "    p_arr = np.empty(B); r_arr = np.empty(B); f_arr = np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = RNG.integers(0, n, size=n)\n",
    "        boot_ids = ids_np[idx].tolist()\n",
    "        TP,FP,FN = _micro_counts(pred_sets, true_sets, boot_ids)\n",
    "        p,r,f = _micro_metrics_from_counts(TP,FP,FN)\n",
    "        p_arr[b], r_arr[b], f_arr[b] = p,r,f\n",
    "        if progress_cb: progress_cb(1)\n",
    "    def _ci(a):\n",
    "        lo, hi = np.percentile(a, [2.5,97.5]); return lo, hi, float(a.std(ddof=1))\n",
    "    p_lo,p_hi,p_sd = _ci(p_arr); r_lo,r_hi,r_sd = _ci(r_arr); f_lo,f_hi,f_sd = _ci(f_arr)\n",
    "    return {\"precision\":(p0,p_lo,p_hi,p_sd), \"recall\":(r0,r_lo,r_hi,r_sd), \"f1_micro\":(f0,f_lo,f_hi,f_sd)}\n",
    "\n",
    "def _paired_bootstrap_delta_selfref(predA: Dict[str, Set[str]], trueA: Dict[str, Set[str]],\n",
    "                                    predB: Dict[str, Set[str]], trueB: Dict[str, Set[str]],\n",
    "                                    ids: List[str], B:int, progress_cb=None):\n",
    "    \"\"\"Paired bootstrap over SAME note IDs; A and B each use their own truths.\n",
    "       Returns deltas and one-sided p-values for Precision, Recall, and F1 (test A > B).\"\"\"\n",
    "    # point\n",
    "    TP_A,FP_A,FN_A = _micro_counts(predA, trueA, ids)\n",
    "    TP_B,FP_B,FN_B = _micro_counts(predB, trueB, ids)\n",
    "    pA,rA,fA = _micro_metrics_from_counts(TP_A,FP_A,FN_A)\n",
    "    pB,rB,fB = _micro_metrics_from_counts(TP_B,FP_B,FN_B)\n",
    "\n",
    "    n = len(ids); ids_np = np.array(ids, dtype=object)\n",
    "    dp = np.empty(B); dr = np.empty(B); df = np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = RNG.integers(0, n, size=n)\n",
    "        boot_ids = ids_np[idx].tolist()\n",
    "        TP_A,FP_A,FN_A = _micro_counts(predA, trueA, boot_ids)\n",
    "        TP_B,FP_B,FN_B = _micro_counts(predB, trueB, boot_ids)\n",
    "        pA,rA,fA = _micro_metrics_from_counts(TP_A,FP_A,FN_A)\n",
    "        pB,rB,fB = _micro_metrics_from_counts(TP_B,FP_B,FN_B)\n",
    "        dp[b], dr[b], df[b] = (pA-pB), (rA-rB), (fA-fB)\n",
    "        if progress_cb: progress_cb(1)\n",
    "\n",
    "    def _ci(a):\n",
    "        lo, hi = np.percentile(a, [2.5,97.5]); return float(a.mean()), float(lo), float(hi), float(a.std(ddof=1))\n",
    "\n",
    "    # One-sided p-values for \"A > B\" (i.e., delta <= 0 is evidence against)\n",
    "    p_one_sided_precision = float((dp <= 0).mean())\n",
    "    p_one_sided_recall    = float((dr <= 0).mean())\n",
    "    p_one_sided_f1        = float((df <= 0).mean())\n",
    "\n",
    "    return {\n",
    "        \"d_precision\": _ci(dp),\n",
    "        \"d_recall\":    _ci(dr),\n",
    "        \"d_f1_micro\":  _ci(df),\n",
    "        \"p_one_sided_precision\": p_one_sided_precision,\n",
    "        \"p_one_sided_recall\":    p_one_sided_recall,\n",
    "        \"p_one_sided_f1\":        p_one_sided_f1,\n",
    "    }\n",
    "\n",
    "# ============================\n",
    "# 4) Main\n",
    "# ============================\n",
    "def load_run(folder: Path):\n",
    "    val_df  = _read_feather(folder/\"predictions_val.feather\")\n",
    "    test_df = _read_feather(folder/\"predictions_test.feather\")\n",
    "    val_df[\"target\"]  = val_df[\"target\"].apply(ensure_list)\n",
    "    test_df[\"target\"] = test_df[\"target\"].apply(ensure_list)\n",
    "    return val_df, test_df\n",
    "\n",
    "def main():\n",
    "    print(f\"==> Stage 1/4: Loading & tuning (self-referenced)\")\n",
    "    per_arch = {}\n",
    "    total_runs = 0\n",
    "    for arch, paths in models_labelset_paths.items():\n",
    "        loaded = {}\n",
    "        for labelset, folder_name in paths.items():\n",
    "            folder = BASE_DIR / folder_name\n",
    "            if not folder.exists():\n",
    "                print(f\"   [WARN] Missing folder: {folder}\")\n",
    "                continue\n",
    "            total_runs += 1\n",
    "            val_df, test_df = load_run(folder)\n",
    "            t0 = time.time()\n",
    "            thr = _sweep_threshold_for_val(val_df)  # tuned on this run's own val targets\n",
    "            print(f\"   Loaded {arch}/{labelset} from {folder.name}: tuned threshold={thr:.6f} (took {time.time()-t0:.1f}s)\")\n",
    "            pred_sets = _pred_sets_from_df(test_df, thr)  # test predictions binarized\n",
    "            true_sets = _targets_dict(test_df)            # this run's OWN test targets\n",
    "            ids = [str(x) for x in test_df[\"_id\"].tolist()]\n",
    "            loaded[labelset] = dict(folder=str(folder), threshold=thr, pred_sets=pred_sets, true_sets=true_sets, ids=ids)\n",
    "        if not loaded: continue\n",
    "        # common note ids across available labelsets (so deltas use same notes)\n",
    "        common_ids = None\n",
    "        for k in loaded:\n",
    "            s = set(loaded[k][\"ids\"])\n",
    "            common_ids = s if common_ids is None else (common_ids & s)\n",
    "        per_arch[arch] = {\"loaded\": loaded, \"common_ids\": sorted(list(common_ids)) if common_ids else []}\n",
    "\n",
    "    if not per_arch:\n",
    "        print(\"No runs loaded. Check BASE_DIR and folder names.\")\n",
    "        return\n",
    "\n",
    "    # plan workload\n",
    "    print(\"==> Stage 2/4: Planning bootstrap workload\")\n",
    "    total_boot_iters = 0\n",
    "    for arch, bundle in per_arch.items():\n",
    "        ids = bundle[\"common_ids\"]; runs = bundle[\"loaded\"]\n",
    "        if not ids: continue\n",
    "        total_boot_iters += len(runs) * N_BOOT  # per-model CIs\n",
    "        # paired deltas available whenever both sides exist\n",
    "        if \"EV\" in runs and \"Original\" in runs: total_boot_iters += N_BOOT\n",
    "        if \"ER\" in runs and \"Original\" in runs: total_boot_iters += N_BOOT\n",
    "        if \"EV\" in runs and \"ER\" in runs:       total_boot_iters += N_BOOT\n",
    "    print(f\"   Planned bootstrap iterations: {total_boot_iters}\")\n",
    "    progress = Progress(total=total_boot_iters, label=\"Bootstrapping\", update_every=PROGRESS_UPDATE_EVERY)\n",
    "    progress_cb = progress.update if total_boot_iters > 0 else None\n",
    "\n",
    "    # 3a) per-model self-referenced CIs\n",
    "    print(\"==> Stage 3/4: Per-model self-referenced CIs\")\n",
    "    per_model_records = []\n",
    "    for arch, bundle in per_arch.items():\n",
    "        ids = bundle[\"common_ids\"]; runs = bundle[\"loaded\"]\n",
    "        if not ids:\n",
    "            print(f\"   [INFO] {arch}: no common IDs; skipping.\")\n",
    "            continue\n",
    "        for labelset, run in runs.items():\n",
    "            ci = _bootstrap_ci_single(run[\"pred_sets\"], run[\"true_sets\"], ids, B=N_BOOT, progress_cb=progress_cb)\n",
    "            per_model_records.append({\n",
    "                \"architecture\": arch,\n",
    "                \"trained_on\": labelset,\n",
    "                \"folder\": run[\"folder\"],\n",
    "                \"threshold\": run[\"threshold\"],\n",
    "                \"precision\": ci[\"precision\"][0], \"precision_lo\": ci[\"precision\"][1], \"precision_hi\": ci[\"precision\"][2],\n",
    "                \"recall\":    ci[\"recall\"][0],    \"recall_lo\":    ci[\"recall\"][1],    \"recall_hi\":    ci[\"recall\"][2],\n",
    "                \"f1_micro\":  ci[\"f1_micro\"][0],  \"f1_lo\":        ci[\"f1_micro\"][1],  \"f1_hi\":        ci[\"f1_micro\"][2],\n",
    "            })\n",
    "    per_model_df = pd.DataFrame(per_model_records).sort_values([\"architecture\",\"trained_on\"])\n",
    "\n",
    "    # 3b) paired deltas (still self-referenced per side), resampling the same notes\n",
    "    print(\"\\n==> Stage 4/4: Paired deltas (self-referenced on each side)\")\n",
    "    delta_records = []\n",
    "    for arch, bundle in per_arch.items():\n",
    "        ids = bundle[\"common_ids\"]; runs = bundle[\"loaded\"]\n",
    "        if not ids: continue\n",
    "        def add_delta(nameA, nameB, label):\n",
    "            d = _paired_bootstrap_delta_selfref(\n",
    "                runs[nameA][\"pred_sets\"], runs[nameA][\"true_sets\"],\n",
    "                runs[nameB][\"pred_sets\"], runs[nameB][\"true_sets\"],\n",
    "                ids, B=N_BOOT, progress_cb=progress_cb)\n",
    "            delta_records.append({\n",
    "                \"architecture\": arch, \"comparison\": label,\n",
    "                \"d_precision\": d[\"d_precision\"][0], \"d_precision_lo\": d[\"d_precision\"][1], \"d_precision_hi\": d[\"d_precision\"][2],\n",
    "                \"d_recall\":    d[\"d_recall\"][0],    \"d_recall_lo\":    d[\"d_recall\"][1],    \"d_recall_hi\":    d[\"d_recall\"][2],\n",
    "                \"d_f1\":        d[\"d_f1_micro\"][0],  \"d_f1_lo\":        d[\"d_f1_micro\"][1],  \"d_f1_hi\":        d[\"d_f1_micro\"][2],\n",
    "                \"p_one_sided_precision\": d[\"p_one_sided_precision\"],\n",
    "                \"p_one_sided_recall\":    d[\"p_one_sided_recall\"],\n",
    "                \"p_one_sided_f1\":        d[\"p_one_sided_f1\"],\n",
    "            })\n",
    "        if \"EV\" in runs and \"Original\" in runs: add_delta(\"EV\",\"Original\",\"EV - Original (self-ref)\")\n",
    "        if \"ER\" in runs and \"Original\" in runs: add_delta(\"ER\",\"Original\",\"ER - Original (self-ref)\")\n",
    "        if \"EV\" in runs and \"ER\" in runs:       add_delta(\"EV\",\"ER\",\"EV - ER (self-ref)\")\n",
    "    delta_df = pd.DataFrame(delta_records).sort_values([\"architecture\",\"comparison\"])\n",
    "\n",
    "    # save\n",
    "    out_dir = BASE_DIR / \"bootstrap_results_selfref\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    per_model_df.to_csv(out_dir/\"per_model_ci.csv\", index=False)\n",
    "    delta_df.to_csv(out_dir/\"paired_deltas_ci.csv\", index=False)\n",
    "\n",
    "    print(\"\\n==> Done. Saved results to:\")\n",
    "    print(f\"   {out_dir/'per_model_ci.csv'}\")\n",
    "    print(f\"   {out_dir/'paired_deltas_ci.csv'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08d44232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  architecture trained_on    folder  threshold  precision  precision_lo  \\\n",
      "0         CAML         ER  b1tl66nj       0.28   0.466273      0.453448   \n",
      "1         CAML         EV  05cvm0pf       0.28   0.492162      0.479733   \n",
      "2         CAML   Original  p9hj5xe1       0.24   0.440739      0.429978   \n",
      "3          CNN         ER  mlpgnhsf       0.38   0.248779      0.185121   \n",
      "4          CNN         EV  qj24zl5u       0.34   0.271605      0.200683   \n",
      "\n",
      "   precision_hi    recall  recall_lo  recall_hi  f1_micro     f1_lo     f1_hi  \n",
      "0      0.479666  0.365097   0.353427   0.376659  0.409528  0.398828  0.420742  \n",
      "1      0.505628  0.393753   0.382122   0.405519  0.437492  0.426606  0.448394  \n",
      "2      0.451846  0.364269   0.353817   0.374460  0.398872  0.389028  0.408493  \n",
      "3      0.326971  0.198369   0.188234   0.208049  0.220733  0.190648  0.248949  \n",
      "4      0.353367  0.228172   0.217953   0.237821  0.248001  0.213074  0.278574  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_micro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <th>trained_on</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">CAML</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.466</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.492</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.441</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">CNN</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.249</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.272</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.224</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LAAT</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.501</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.512</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MultiResCNN</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.521</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.529</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.424</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">RNN</th>\n",
       "      <th>ER</th>\n",
       "      <td>0.348</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>0.381</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.334</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision  recall  f1_micro\n",
       "architecture trained_on                             \n",
       "CAML         ER              0.466   0.365     0.410\n",
       "             EV              0.492   0.394     0.437\n",
       "             Original        0.441   0.364     0.399\n",
       "CNN          ER              0.249   0.198     0.221\n",
       "             EV              0.272   0.228     0.248\n",
       "             Original        0.224   0.244     0.234\n",
       "LAAT         ER              0.501   0.395     0.442\n",
       "             EV              0.512   0.378     0.435\n",
       "             Original        0.478   0.356     0.408\n",
       "MultiResCNN  ER              0.521   0.304     0.384\n",
       "             EV              0.529   0.362     0.429\n",
       "             Original        0.424   0.321     0.366\n",
       "RNN          ER              0.348   0.227     0.275\n",
       "             EV              0.381   0.251     0.303\n",
       "             Original        0.334   0.259     0.292"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "per_model = pd.read_csv(\"bootstrap_results_selfref/per_model_ci.csv\")\n",
    "print(per_model.head())\n",
    "\n",
    "# Quick summary table by architecture\n",
    "summary = (\n",
    "    per_model.groupby([\"architecture\",\"trained_on\"])\n",
    "    [[\"precision\",\"recall\",\"f1_micro\"]].mean()\n",
    "    .round(3)\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ca708a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures\\fixed_reference_all_metrics.png figures\\self_reference_all_metrics.png figures\\combined_side_by_side.png\n"
     ]
    }
   ],
   "source": [
    "# Plot ΔPrecision / ΔF1 / ΔRecall with 95% CIs + p-value stars (Precision/F1/Recall)\n",
    "# Groups: EV−OM (top) then ER−OM (bottom), each with a single rotated label + square bracket\n",
    "# Saves: figures/fixed_reference_all_metrics.png\n",
    "#        figures/self_reference_all_metrics.png\n",
    "#        figures/combined_side_by_side.png\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "# ---------- config ----------\n",
    "fixed_dir = \"bootstrap_results\"\n",
    "self_dir  = \"bootstrap_results_selfref\"\n",
    "out_dir   = \"figures\"\n",
    "\n",
    "ARCH_ORDER = [\"LAAT\", \"CAML\", \"MultiResCNN\", \"RNN\", \"CNN\"]  # enforced within each block\n",
    "COMP_ORDER = [\"EV−OM\", \"ER−OM\"]                              # block order top→bottom\n",
    "\n",
    "# Color/marker settings: keep colored, but ensure B/W differentiability via darker hues + black edges\n",
    "COLOR_MAP  = {\n",
    "    \"Precision\": \"#377eb8\",  # dark-ish blue\n",
    "    \"F1\":        \"#e41a1c\",  # dark-ish red\n",
    "    \"Recall\":    \"#4daf4a\",  # dark-ish green\n",
    "}\n",
    "MARKER_MAP = {\"Precision\": \"s\", \"F1\": \"o\", \"Recall\": \"D\"}\n",
    "Y_OFFSETS  = {\"Precision\": +0.25, \"F1\": 0.00, \"Recall\": -0.25}  # Precision up, F1 mid, Recall down\n",
    "\n",
    "# Legend controls (tweak these if you want different sizes/layout later)\n",
    "LEGEND_MARKERSCALE = 1.3\n",
    "LEGEND_FONT_SIZE   = 6\n",
    "LEGEND_NCOL        = 3\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def p_to_stars(p):\n",
    "    try:\n",
    "        if np.isnan(p): return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p <= 0.001: return \"***\"\n",
    "    if p <= 0.01:  return \"**\"\n",
    "    if p <= 0.05:  return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "def _norm_comp_label(raw):\n",
    "    \"\"\"Map both fixed and self-ref CSV comparison strings to EV−OM / ER−OM (no '(self)').\"\"\"\n",
    "    raw = str(raw)\n",
    "    if raw.startswith(\"EV - Original\"): return \"EV−OM\"\n",
    "    if raw.startswith(\"ER - Original\"): return \"ER−OM\"\n",
    "    return raw\n",
    "\n",
    "def load_paired_deltas_csv(csv_path):\n",
    "    \"\"\"Load paired_deltas_ci.csv and reshape to long format with p-values per metric.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Keep only EV-OM and ER-OM comparisons (both fixed and self files may contain self-ref suffix)\n",
    "    df = df[df[\"comparison\"].str.startswith((\"EV - Original\", \"ER - Original\"))].copy()\n",
    "    df[\"Comparison\"] = df[\"comparison\"].map(_norm_comp_label)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        arch = r[\"architecture\"]\n",
    "        comp = r[\"Comparison\"]\n",
    "        rows.append(dict(Architecture=arch, Comparison=comp, Metric=\"Precision\",\n",
    "                         Delta=r[\"d_precision\"], Lo=r[\"d_precision_lo\"], Hi=r[\"d_precision_hi\"],\n",
    "                         p=r.get(\"p_one_sided_precision\", np.nan)))\n",
    "        rows.append(dict(Architecture=arch, Comparison=comp, Metric=\"F1\",\n",
    "                         Delta=r[\"d_f1\"], Lo=r[\"d_f1_lo\"], Hi=r[\"d_f1_hi\"],\n",
    "                         p=r.get(\"p_one_sided_f1\", np.nan)))\n",
    "        rows.append(dict(Architecture=arch, Comparison=comp, Metric=\"Recall\",\n",
    "                         Delta=r[\"d_recall\"], Lo=r[\"d_recall_lo\"], Hi=r[\"d_recall_hi\"],\n",
    "                         p=r.get(\"p_one_sided_recall\", np.nan)))\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"Comparison\"]   = pd.Categorical(out[\"Comparison\"],   categories=COMP_ORDER, ordered=True)\n",
    "    out[\"Architecture\"] = pd.Categorical(out[\"Architecture\"], categories=ARCH_ORDER, ordered=True)\n",
    "    out = out.sort_values([\"Comparison\", \"Architecture\", \"Metric\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def plot_forest_on_ax(ax, df_long, title=None, xlim=None, add_legend=False):\n",
    "    \"\"\"Draw one panel with:\n",
    "       - EV−OM block (top) then ER−OM (bottom)\n",
    "       - single rotated label + square bracket per block\n",
    "       - p-value stars for Precision, F1, Recall (smaller & closer)\n",
    "       - thick black separator between blocks\n",
    "       - y-tick labels = model names only, vertical\n",
    "    \"\"\"\n",
    "    df = df_long.copy()\n",
    "\n",
    "    # Build row order = for each comp (EV−OM then ER−OM), for each arch in fixed order\n",
    "    unique_rows = (\n",
    "        df.drop_duplicates(subset=[\"Comparison\",\"Architecture\"])[[\"Comparison\",\"Architecture\"]]\n",
    "          .sort_values([\"Comparison\",\"Architecture\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    unique_rows[\"RowKey\"] = unique_rows[\"Comparison\"].astype(str) + \" · \" + unique_rows[\"Architecture\"].astype(str)\n",
    "    row_keys = unique_rows[\"RowKey\"].tolist()\n",
    "\n",
    "    # y-base: reverse so first is top\n",
    "    y_base_map = {rk: i for i, rk in enumerate(row_keys[::-1])}\n",
    "\n",
    "    # Attach y_base to each metric row\n",
    "    df[\"RowKey\"] = df[\"Comparison\"].astype(str) + \" · \" + df[\"Architecture\"].astype(str)\n",
    "    df[\"y_base\"] = df[\"RowKey\"].map(y_base_map)\n",
    "\n",
    "    # Plot metrics with offsets and p-value stars\n",
    "    for metric in [\"Precision\", \"F1\", \"Recall\"]:\n",
    "        sub = df[df[\"Metric\"] == metric]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        y = sub[\"y_base\"].values + Y_OFFSETS[metric]\n",
    "        x = sub[\"Delta\"].values\n",
    "        xerr = np.vstack([x - sub[\"Lo\"].values, sub[\"Hi\"].values - x])\n",
    "        ax.errorbar(\n",
    "            x, y, xerr=xerr,\n",
    "            fmt=MARKER_MAP[metric],\n",
    "            markersize=7,               # bigger symbols\n",
    "            mfc=COLOR_MAP[metric],      # colored face\n",
    "            mec=\"black\", mew=0.7,       # black edge for B/W clarity\n",
    "            capsize=0,\n",
    "            linestyle=\"None\", label=metric,\n",
    "            color=COLOR_MAP[metric], ecolor=\"black\",  # black CI edges for B/W\n",
    "            elinewidth=1.0, zorder=3\n",
    "        )\n",
    "        # p-value stars\n",
    "        for xi, yi, pi in zip(x, y, sub[\"p\"].values):\n",
    "            stars = p_to_stars(pi)\n",
    "            if stars:\n",
    "                ax.text(\n",
    "                    xi, yi + 0.03,\n",
    "                    stars,\n",
    "                    fontsize=7,\n",
    "                    ha=\"center\", va=\"bottom\",\n",
    "                    color=\"black\", zorder=4\n",
    "                )\n",
    "\n",
    "    # Symmetric y-limits so top/bottom gap = inter-row gap\n",
    "    if df[\"y_base\"].empty:\n",
    "        return ax\n",
    "    ybases = sorted(df[\"y_base\"].unique())\n",
    "    ax.set_ylim(ybases[0] - 0.5, ybases[-1] + 0.5)\n",
    "\n",
    "    # x-limits & separators\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    x0, x1 = ax.get_xlim()\n",
    "    for yb in ybases[:-1]:\n",
    "        ax.hlines(yb + 0.5, x0, x1, colors=\"#dddddd\", linestyles=\"-\", linewidth=0.8, zorder=1)\n",
    "\n",
    "    # Vertical reference at 0\n",
    "    ax.axvline(0, linestyle=\"--\", color=\"gray\", linewidth=1, zorder=1)\n",
    "\n",
    "    # y ticks & labels — model names ONLY, vertical\n",
    "    display_rows = unique_rows.iloc[::-1].reset_index(drop=True)  # top→bottom display\n",
    "    display_rows[\"RowKey\"] = display_rows[\"Comparison\"].astype(str) + \" · \" + display_rows[\"Architecture\"].astype(str)\n",
    "    yticks  = [y_base_map[rk] for rk in row_keys[::-1]]\n",
    "    ylabels = display_rows[\"Architecture\"].tolist()  # only model name\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(ylabels, rotation=90, va=\"center\", ha=\"center\", fontsize=6.5)  # vertical, centered\n",
    "\n",
    "    # Thick black separator between EV−OM and ER−OM blocks\n",
    "    display_comp = display_rows[\"Comparison\"].tolist()\n",
    "    display_base = [y_base_map[rk] for rk in display_rows[\"RowKey\"]]\n",
    "    for i in range(len(display_rows) - 1):\n",
    "        if display_comp[i] != display_comp[i + 1]:\n",
    "            y_boundary = display_base[i] + 0.5\n",
    "            ax.hlines(y_boundary, x0, x1, colors=\"black\", linewidth=1.8, zorder=2)\n",
    "\n",
    "    # Single rotated bracketed block label in LEFT MARGIN for each comparison\n",
    "    trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "    x_bracket = -0.055\n",
    "    x_text    = -0.075\n",
    "    bracket_tick = 0.015\n",
    "\n",
    "    i = 0\n",
    "    total = len(display_rows)\n",
    "    while i < total:\n",
    "        comp_i = display_comp[i]\n",
    "        j = i\n",
    "        while j < total and display_comp[j] == comp_i:\n",
    "            j += 1\n",
    "        bases = [display_base[k] for k in range(i, j)]\n",
    "        if bases:\n",
    "            y_top    = max(bases) + 0.25\n",
    "            y_bottom = min(bases) - 0.25\n",
    "            y_mid    = 0.5 * (y_top + y_bottom)\n",
    "\n",
    "            ax.plot([x_bracket, x_bracket], [y_bottom, y_top], color=\"black\", lw=1.2,\n",
    "                    transform=trans, clip_on=False, zorder=2)\n",
    "            ax.plot([x_bracket, x_bracket + bracket_tick], [y_top, y_top], color=\"black\",\n",
    "                    lw=1.2, transform=trans, clip_on=False, zorder=2)\n",
    "            ax.plot([x_bracket, x_bracket + bracket_tick], [y_bottom, y_bottom], color=\"black\",\n",
    "                    lw=1.2, transform=trans, clip_on=False, zorder=2)\n",
    "\n",
    "            ax.text(x_text, y_mid, comp_i, rotation=90, ha=\"center\", va=\"center\",\n",
    "                    fontsize=10, color=\"black\", weight=\"bold\",\n",
    "                    transform=trans, clip_on=False, zorder=5)\n",
    "        i = j\n",
    "\n",
    "    ax.set_xlabel(\"Δ (95% CI - Confidence Interval)\", fontsize=7.5)\n",
    "    if title:\n",
    "        ax.set_title(title, pad=8)\n",
    "\n",
    "    if add_legend:\n",
    "        leg = ax.legend(\n",
    "            loc=\"lower right\",\n",
    "            bbox_to_anchor=(0.2, -0.23),\n",
    "            ncol=LEGEND_NCOL,\n",
    "            frameon=False,\n",
    "            markerscale=LEGEND_MARKERSCALE,\n",
    "        )\n",
    "        for t in leg.get_texts():\n",
    "            t.set_fontsize(LEGEND_FONT_SIZE)\n",
    "\n",
    "    # Wider left margin for bracket + label; small bottom for legend\n",
    "    ax.figure.subplots_adjust(left=0.25, bottom=0.16)\n",
    "    return ax\n",
    "\n",
    "# ----- load data -----\n",
    "fixed_long = load_paired_deltas_csv(os.path.join(fixed_dir, \"paired_deltas_ci.csv\"))\n",
    "self_long  = load_paired_deltas_csv(os.path.join(self_dir,  \"paired_deltas_ci.csv\"))\n",
    "\n",
    "# unified x-range for all panels\n",
    "xmin = min(fixed_long[\"Lo\"].min(), self_long[\"Lo\"].min())\n",
    "xmax = max(fixed_long[\"Hi\"].max(), self_long[\"Hi\"].max())\n",
    "xlim = (float(xmin - 0.01), float(xmax + 0.01))\n",
    "\n",
    "# Convenience: subsets per comparison\n",
    "fixed_ev = fixed_long[fixed_long[\"Comparison\"] == \"EV−OM\"]\n",
    "fixed_er = fixed_long[fixed_long[\"Comparison\"] == \"ER−OM\"]\n",
    "self_ev  = self_long[self_long[\"Comparison\"]  == \"EV−OM\"]\n",
    "self_er  = self_long[self_long[\"Comparison\"]  == \"ER−OM\"]\n",
    "\n",
    "# ----- fixed-reference (now side-by-side EV−OM | ER−OM) -----\n",
    "fig1, axes1 = plt.subplots(1, 2, figsize=(14, 3.6), sharex=True)\n",
    "plot_forest_on_ax(axes1[0], fixed_ev, title=\"\", xlim=xlim, add_legend=False)\n",
    "plot_forest_on_ax(axes1[1], fixed_er, title=\"\", xlim=xlim, add_legend=True)\n",
    "fig1.suptitle(\"Fixed-reference: ΔPrecision / ΔF1 / ΔRecall\", y=0.98)\n",
    "fig1.subplots_adjust(wspace=0.25, bottom=0.16)\n",
    "fig1.savefig(os.path.join(out_dir, \"fixed_reference_all_metrics.png\"), dpi=300, bbox_inches=\"tight\", pad_inches=0.08)\n",
    "plt.close(fig1)\n",
    "\n",
    "# ----- self-referenced (now side-by-side EV−OM | ER−OM) -----\n",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(14, 3.6), sharex=True)\n",
    "plot_forest_on_ax(axes2[0], self_ev,  title=\"\", xlim=xlim, add_legend=False)\n",
    "plot_forest_on_ax(axes2[1], self_er,  title=\"\", xlim=xlim, add_legend=True)\n",
    "fig2.suptitle(\"\", y=0.98)\n",
    "fig2.subplots_adjust(wspace=0.25, bottom=0.16)\n",
    "fig2.savefig(os.path.join(out_dir, \"self_reference_all_metrics.png\"), dpi=300, bbox_inches=\"tight\", pad_inches=0.08)\n",
    "plt.close(fig2)\n",
    "\n",
    "# ----- combined now STACKED vertically (top row = fixed side-by-side, bottom row = self side-by-side) -----\n",
    "fig = plt.figure(figsize=(14, 13.6))\n",
    "gs = fig.add_gridspec(nrows=2, ncols=2, hspace=0.35, wspace=0.25)\n",
    "\n",
    "ax_tl = fig.add_subplot(gs[0, 0])\n",
    "ax_tr = fig.add_subplot(gs[0, 1])\n",
    "ax_bl = fig.add_subplot(gs[1, 0])\n",
    "ax_br = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# top row: fixed\n",
    "plot_forest_on_ax(ax_tl, fixed_ev, title=\"(A) Fixed-reference — EV−OM\", xlim=xlim, add_legend=False)\n",
    "plot_forest_on_ax(ax_tr, fixed_er, title=\"(B) Fixed-reference — ER−OM\", xlim=xlim, add_legend=True)\n",
    "\n",
    "# bottom row: self\n",
    "plot_forest_on_ax(ax_bl, self_ev,  title=\"(C) Self-referenced — EV−OM\", xlim=xlim, add_legend=False)\n",
    "plot_forest_on_ax(ax_br, self_er,  title=\"(D) Self-referenced — ER−OM\", xlim=xlim, add_legend=True)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.16)\n",
    "fig.savefig(os.path.join(out_dir, \"combined_side_by_side.png\"), dpi=300, bbox_inches=\"tight\", pad_inches=0.08)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved:\",\n",
    "      os.path.join(out_dir, \"fixed_reference_all_metrics.png\"),\n",
    "      os.path.join(out_dir, \"self_reference_all_metrics.png\"),\n",
    "      os.path.join(out_dir, \"combined_side_by_side.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8d9c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures/per_model_fixed_reference.png figures/per_model_self_reference.png figures/per_model_combined_side_by_side.png\n"
     ]
    }
   ],
   "source": [
    "# Per-model forest plots with OM→EV→ER order and thick black separators between architectures\n",
    "# Inputs:\n",
    "#   bootstrap_results/per_model_ci.csv\n",
    "#   bootstrap_results_selfref/per_model_ci.csv\n",
    "# Outputs:\n",
    "#   figures/per_model_fixed_reference.png\n",
    "#   figures/per_model_self_reference.png\n",
    "#   figures/per_model_combined_side_by_side.png\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- config ----------\n",
    "fixed_dir = \"bootstrap_results\"\n",
    "self_dir  = \"bootstrap_results_selfref\"\n",
    "out_dir   = \"figures\"\n",
    "\n",
    "ARCH_ORDER  = [\"LAAT\", \"CAML\", \"MultiResCNN\", \"RNN\", \"CNN\"]   # enforce this always\n",
    "TRAIN_ORDER = [\"OM\", \"EV\", \"ER\"]  # row order within each architecture\n",
    "\n",
    "# light colors & markers; vertical offsets so points don’t overlap\n",
    "COLOR_MAP  = {\"Precision\": \"#9ecae1\", \"F1\": \"#fc9272\", \"Recall\": \"#a1d99b\"}  # blue / red / green (light)\n",
    "MARKER_MAP = {\"Precision\": \"s\", \"F1\": \"o\", \"Recall\": \"D\"}\n",
    "Y_OFFSETS  = {\"Precision\": +0.25, \"F1\": 0.00, \"Recall\": -0.25}  # Precision up, F1 mid, Recall down\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_per_model_ci(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load per_model_ci.csv and reshape to long format (no p-values here).\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # rename Original->OM for display and ordering\n",
    "    df[\"trained_on\"] = df[\"trained_on\"].replace({\"Original\": \"OM\"})\n",
    "    # build long rows for three metrics\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        arch = r[\"architecture\"]\n",
    "        tr   = r[\"trained_on\"]\n",
    "        rows.append(dict(Architecture=arch, TrainedOn=tr, Metric=\"Precision\",\n",
    "                         Val=r[\"precision\"], Lo=r[\"precision_lo\"], Hi=r[\"precision_hi\"]))\n",
    "        rows.append(dict(Architecture=arch, TrainedOn=tr, Metric=\"F1\",\n",
    "                         Val=r[\"f1_micro\"], Lo=r[\"f1_lo\"], Hi=r[\"f1_hi\"]))\n",
    "        rows.append(dict(Architecture=arch, TrainedOn=tr, Metric=\"Recall\",\n",
    "                         Val=r[\"recall\"], Lo=r[\"recall_lo\"], Hi=r[\"recall_hi\"]))\n",
    "    out = pd.DataFrame(rows)\n",
    "    # enforce ordering\n",
    "    out[\"Architecture\"] = pd.Categorical(out[\"Architecture\"], categories=ARCH_ORDER, ordered=True)\n",
    "    out[\"TrainedOn\"]   = pd.Categorical(out[\"TrainedOn\"],   categories=TRAIN_ORDER, ordered=True)\n",
    "    out = out.sort_values([\"Architecture\",\"TrainedOn\",\"Metric\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def multiline_label(arch, trained_on):\n",
    "    # two lines + tiny spacer keeps the three markers visually grouped\n",
    "    return f\"{arch}\\n{trained_on}\\n\"\n",
    "\n",
    "def plot_forest_on_ax(ax, df_long: pd.DataFrame, title=None, xlim=None, add_legend=False):\n",
    "    \"\"\"Per-model forest plot with vertical arch labels + square bracket in left margin (tight symmetric top/bottom).\"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.transforms as mtransforms\n",
    "\n",
    "    df = df_long.copy()\n",
    "    df[\"RowKey\"] = df[\"Architecture\"].astype(str) + \" · \" + df[\"TrainedOn\"].astype(str)\n",
    "    rows = (\n",
    "        df.drop_duplicates(subset=[\"RowKey\"])[[\"Architecture\", \"TrainedOn\", \"RowKey\"]]\n",
    "        .sort_values([\"Architecture\", \"TrainedOn\"])\n",
    "    )\n",
    "    row_keys = rows[\"RowKey\"].tolist()\n",
    "\n",
    "    # y-base positions (reverse so top is first)\n",
    "    y_base_map = {rk: i for i, rk in enumerate(row_keys[::-1])}\n",
    "    df[\"y_base\"] = df[\"RowKey\"].map(y_base_map)\n",
    "\n",
    "    # ---- plot metrics (Precision↑, F1 mid, Recall↓) ----\n",
    "    color_map = {\"Precision\": \"#9ecae1\", \"F1\": \"#fc9272\", \"Recall\": \"#a1d99b\"}\n",
    "    marker_map = {\"Precision\": \"s\", \"F1\": \"o\", \"Recall\": \"D\"}\n",
    "    offsets    = {\"Precision\": +0.25, \"F1\": 0.0, \"Recall\": -0.25}\n",
    "\n",
    "    for metric in [\"Precision\", \"F1\", \"Recall\"]:\n",
    "        sub = df[df[\"Metric\"] == metric]\n",
    "        y = sub[\"y_base\"].values + offsets[metric]\n",
    "        x = sub[\"Val\"].values\n",
    "        xerr = np.vstack([x - sub[\"Lo\"].values, sub[\"Hi\"].values - x])\n",
    "        ax.errorbar(\n",
    "            x, y, xerr=xerr,\n",
    "            fmt=marker_map[metric], color=color_map[metric],\n",
    "            ecolor=color_map[metric], markersize=5, capsize=0,\n",
    "            linestyle=\"None\", elinewidth=1.2, zorder=3, label=metric\n",
    "        )\n",
    "\n",
    "    # ---- set symmetric y-limits so top/bottom gap = inter-row gap ----\n",
    "    ybases = sorted(df[\"y_base\"].unique())  # e.g., [0,1,2,...,N-1]\n",
    "    ax.set_ylim(ybases[0] - 0.5, ybases[-1] + 0.5)  # <<< key line for equal top/bottom spacing\n",
    "\n",
    "    # ---- thin separators between ticks (behind points) ----\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    x0, x1 = ax.get_xlim()\n",
    "    for yb in ybases[:-1]:\n",
    "        ax.hlines(yb + 0.5, x0, x1, colors=\"#dddddd\", linestyles=\"-\", linewidth=0.8, zorder=1)\n",
    "\n",
    "    # ---- thick black separators between architectures (one tick up) ----\n",
    "    display_rows = rows.iloc[::-1].reset_index(drop=True)  # top→bottom\n",
    "    display_arch = display_rows[\"Architecture\"].tolist()\n",
    "    display_base = [y_base_map[rk] for rk in display_rows[\"RowKey\"]]\n",
    "    for i in range(len(display_rows) - 1):\n",
    "        if display_arch[i] != display_arch[i + 1]:\n",
    "            y_boundary = display_base[i] + 0.5  # one tick up\n",
    "            ax.hlines(y_boundary, x0, x1, colors=\"black\", linewidth=1.8, zorder=2)\n",
    "\n",
    "    # ---- y-axis ticks: only labelset names ----\n",
    "    # ---- y-axis ticks: only labelset names (centered on ticks) ----\n",
    "    yticks  = [y_base_map[rk] for rk in row_keys[::-1]]\n",
    "    # REMOVE the trailing '\\n' — this is what was pushing labels upward\n",
    "    ylabels = [f\"{tr}\" for _, tr, _ in rows.iloc[::-1][[\"Architecture\",\"TrainedOn\",\"RowKey\"]].itertuples(index=False)]\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(ylabels)\n",
    "\n",
    "    # Force vertical centering for safety (some backends ignore va via set_yticklabels)\n",
    "    for lab in ax.get_yticklabels():\n",
    "        lab.set_va(\"center\")\n",
    "        # If your labels still look off on your backend, uncomment the next line:\n",
    "        # lab.set_linespacing(1.0)\n",
    "\n",
    "\n",
    "    # ---- vertical architecture labels + square bracket in LEFT MARGIN ----\n",
    "    # axes-x / data-y transform so y aligns to rows; x in axis fraction (margin)\n",
    "    trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "    # tweak these two to nudge left/right:\n",
    "    x_bracket = -0.067   # bracket spine (axes fraction; make more negative to go further left)\n",
    "    x_text    = -0.085   # model text (a bit left of the bracket)\n",
    "    bracket_tick = 0.015 # tick length on bracket\n",
    "\n",
    "    i = 0\n",
    "    total = len(display_rows)\n",
    "    while i < total:\n",
    "        arch_i = display_arch[i]\n",
    "        j = i\n",
    "        while j < total and display_arch[j] == arch_i:\n",
    "            j += 1\n",
    "        bases = [display_base[k] for k in range(i, j)]\n",
    "        if bases:\n",
    "            # Span exactly from top Precision (+0.25) to bottom Recall (-0.25)\n",
    "            y_top    = max(bases) + 0.3   # <<< was +0.6; corrected for symmetry\n",
    "            y_bottom = min(bases) - 0.25\n",
    "            y_mid    = 0.5 * (y_top + y_bottom)\n",
    "\n",
    "            # vertical bracket\n",
    "            ax.plot([x_bracket, x_bracket], [y_bottom, y_top], color=\"black\", lw=1.2,\n",
    "                    transform=trans, clip_on=False, zorder=2)\n",
    "            # top & bottom bracket ticks\n",
    "            ax.plot([x_bracket, x_bracket + bracket_tick], [y_top, y_top], color=\"black\",\n",
    "                    lw=1.2, transform=trans, clip_on=False, zorder=2)\n",
    "            ax.plot([x_bracket, x_bracket + bracket_tick], [y_bottom, y_bottom], color=\"black\",\n",
    "                    lw=1.2, transform=trans, clip_on=False, zorder=2)\n",
    "\n",
    "            # vertical model name centered on the block\n",
    "            ax.text(x_text, y_mid, arch_i, rotation=90, ha=\"center\", va=\"center\",\n",
    "                    fontsize=10, color=\"black\", weight=\"bold\",\n",
    "                    transform=trans, clip_on=False, zorder=5)\n",
    "        i = j\n",
    "\n",
    "    # ---- labels & legend ----\n",
    "    ax.set_xlabel(\"Score (95% CI)\")\n",
    "    if title:\n",
    "        ax.set_title(title, pad=8)\n",
    "\n",
    "    if add_legend:\n",
    "        leg = ax.legend(\n",
    "            loc=\"upper center\", bbox_to_anchor=(0.5, -0.08),\n",
    "            ncol=3, frameon=False, handletextpad=0.8\n",
    "        )\n",
    "        for t in leg.get_texts():\n",
    "            t.set_fontsize(9)\n",
    "\n",
    "    # enough left/bottom room for bracket + legend; adjust if needed\n",
    "    ax.figure.subplots_adjust(left=0.26, bottom=0.16)\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- load per-model CIs -----\n",
    "fixed_ci = load_per_model_ci(os.path.join(fixed_dir, \"per_model_ci.csv\"))\n",
    "self_ci  = load_per_model_ci(os.path.join(self_dir,  \"per_model_ci.csv\"))\n",
    "\n",
    "# unified x-range for both panels (use [min_lo, max_hi] across both)\n",
    "xmin = min(fixed_ci[\"Lo\"].min(), self_ci[\"Lo\"].min())\n",
    "xmax = max(fixed_ci[\"Hi\"].max(), self_ci[\"Hi\"].max())\n",
    "# pad and clip to [0,1]\n",
    "xpad = 0.01\n",
    "xlim = (float(max(0.0, xmin - xpad)), float(min(1.0, xmax + xpad)))\n",
    "\n",
    "# ----- individual panels (compact legend, minimal bottom whitespace) -----\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6.6))\n",
    "plot_forest_on_ax(\n",
    "    ax1, fixed_ci,\n",
    "    title=\"Per-model micro metrics vs EV reference: Precision / F1 / Recall (95% CI)\",\n",
    "    xlim=xlim, add_legend=True\n",
    ")\n",
    "fig1.subplots_adjust(bottom=0.16)\n",
    "fig1.savefig(os.path.join(out_dir, \"per_model_fixed_reference.png\"), dpi=300, bbox_inches=\"tight\", pad_inches=0.08)\n",
    "plt.close(fig1)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 6.6))\n",
    "plot_forest_on_ax(\n",
    "    ax2, self_ci,\n",
    "    title=\"Per-model micro metrics (self-referenced): Precision / F1 / Recall (95% CI)\",\n",
    "    xlim=xlim, add_legend=True\n",
    ")\n",
    "fig2.subplots_adjust(bottom=0.16)\n",
    "fig2.savefig(os.path.join(out_dir, \"per_model_self_reference.png\"), dpi=300, bbox_inches=\"tight\", pad_inches=0.08)\n",
    "plt.close(fig2)\n",
    "\n",
    "# ----- combined side-by-side (legend on right panel only) -----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6.6), sharex=True)\n",
    "plot_forest_on_ax(\n",
    "    axes[0], fixed_ci,\n",
    "    title=\"(A) Fixed-reference (vs EV)\",\n",
    "    xlim=xlim, add_legend=False\n",
    ")\n",
    "plot_forest_on_ax(\n",
    "    axes[1], self_ci,\n",
    "    title=\"(B) Self-referenced\",\n",
    "    xlim=xlim, add_legend=True\n",
    ")\n",
    "fig.subplots_adjust(wspace=0.25, bottom=0.16)\n",
    "fig.savefig(os.path.join(out_dir, \"per_model_combined_side_by_side.png\"), dpi=300, bbox_inches=\"tight\", pad_inches=0.08)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved:\",\n",
    "      os.path.join(out_dir, \"per_model_fixed_reference.png\"),\n",
    "      os.path.join(out_dir, \"per_model_self_reference.png\"),\n",
    "      os.path.join(out_dir, \"per_model_combined_side_by_side.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b55b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures/barplot_fixed_reference.png\n",
      "Saved figures/barplot_self_reference.png\n"
     ]
    }
   ],
   "source": [
    "# Grouped bar charts comparing EV / ER / OM per architecture (with 95% CI error bars)\n",
    "# Inputs: bootstrap_results/per_model_ci.csv and bootstrap_results_selfref/per_model_ci.csv\n",
    "# Output: figures/barplot_fixed_reference.png, figures/barplot_self_reference.png, figures/barplot_combined_side_by_side.png\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "fixed_dir = \"bootstrap_results\"\n",
    "self_dir  = \"bootstrap_results_selfref\"\n",
    "out_dir   = \"figures\"\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ARCH_ORDER = [\"LAAT\", \"CAML\", \"MultiResCNN\", \"RNN\", \"CNN\"]\n",
    "LABELSETS  = [\"EV\", \"ER\", \"OM\"]\n",
    "METRICS    = [\"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "COLORS = {\n",
    "    \"Precision\": \"#6baed6\",  # blue\n",
    "    \"Recall\": \"#74c476\",     # green\n",
    "    \"F1\": \"#fb6a4a\"          # red\n",
    "}\n",
    "\n",
    "def load_per_model_ci(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"trained_on\"] = df[\"trained_on\"].replace({\"Original\": \"OM\"})\n",
    "    data = []\n",
    "    for _, r in df.iterrows():\n",
    "        data.append([\"Precision\", r[\"architecture\"], r[\"trained_on\"], r[\"precision\"], r[\"precision_lo\"], r[\"precision_hi\"]])\n",
    "        data.append([\"Recall\",    r[\"architecture\"], r[\"trained_on\"], r[\"recall\"],    r[\"recall_lo\"],    r[\"recall_hi\"]])\n",
    "        data.append([\"F1\",        r[\"architecture\"], r[\"trained_on\"], r[\"f1_micro\"],  r[\"f1_lo\"],        r[\"f1_hi\"]])\n",
    "    df_long = pd.DataFrame(data, columns=[\"Metric\",\"Architecture\",\"LabelSet\",\"Val\",\"Lo\",\"Hi\"])\n",
    "    df_long[\"Architecture\"] = pd.Categorical(df_long[\"Architecture\"], ARCH_ORDER, ordered=True)\n",
    "    df_long[\"LabelSet\"] = pd.Categorical(df_long[\"LabelSet\"], LABELSETS, ordered=True)\n",
    "    return df_long\n",
    "\n",
    "def plot_grouped_bars(df_long, title, save_path):\n",
    "    fig, axes = plt.subplots(1, len(ARCH_ORDER), figsize=(15, 5), sharey=True)\n",
    "    for i, arch in enumerate(ARCH_ORDER):\n",
    "        ax = axes[i]\n",
    "        sub = df_long[df_long[\"Architecture\"] == arch]\n",
    "        bar_width = 0.25\n",
    "        x = np.arange(len(LABELSETS))\n",
    "        offsets = [-bar_width, 0, bar_width]  # precision, recall, f1 positions\n",
    "\n",
    "        for j, metric in enumerate(METRICS):\n",
    "            sm = sub[sub[\"Metric\"] == metric].sort_values(\"LabelSet\")\n",
    "            y = sm[\"Val\"].values\n",
    "            yerr = [y - sm[\"Lo\"].values, sm[\"Hi\"].values - y]\n",
    "            ax.bar(x + offsets[j], y, width=bar_width, color=COLORS[metric],\n",
    "                   label=metric if i == 0 else \"\", yerr=yerr, capsize=3, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(LABELSETS)\n",
    "        ax.set_title(arch)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Score (95% CI)\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=13, y=0.98)\n",
    "    fig.legend(METRICS, loc=\"lower center\", ncol=3, bbox_to_anchor=(0.5, -0.02))\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved {save_path}\")\n",
    "\n",
    "# Load data\n",
    "fixed_df = load_per_model_ci(os.path.join(fixed_dir, \"per_model_ci.csv\"))\n",
    "self_df  = load_per_model_ci(os.path.join(self_dir,  \"per_model_ci.csv\"))\n",
    "\n",
    "# Plot both\n",
    "plot_grouped_bars(fixed_df,\n",
    "                  \"Per-model micro Precision / Recall / F1 (Fixed-reference vs EV)\",\n",
    "                  f\"{out_dir}/barplot_fixed_reference.png\")\n",
    "\n",
    "plot_grouped_bars(self_df,\n",
    "                  \"Per-model micro Precision / Recall / F1 (Self-referenced)\",\n",
    "                  f\"{out_dir}/barplot_self_reference.png\")\n",
    "\n",
    "# Combined (side by side)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 9), sharex=True)\n",
    "for i, (df, title) in enumerate([\n",
    "    (fixed_df, \"(A) Fixed-reference vs EV\"),\n",
    "    (self_df, \"(B) Self-referenced\")\n",
    "]):\n",
    "    for j, arch in enumerate(ARCH_ORDER):\n",
    "        ax = plt.subplot(2, len(ARCH_ORDER), i*len(ARCH_ORDER)+j+1)\n",
    "        sub = df[df[\"Architecture\"] == arch]\n",
    "        bar_width = 0.25\n",
    "        x = np.arange(len(LABELSETS))\n",
    "        offsets = [-bar_width, 0, bar_width]\n",
    "        for k, metric in enumerate(METRICS):\n",
    "            sm = sub[sub[\"Metric\"] == metric].sort_values(\"LabelSet\")\n",
    "            y = sm[\"Val\"].values\n",
    "            yerr = [y - sm[\"Lo\"].values, sm[\"Hi\"].values - y]\n",
    "            ax.bar(x + offsets[k], y, width=bar_width, color=COLORS[metric],\n",
    "                   yerr=yerr, capsize=3, edgecolor=\"black\", linewidth=0.5)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(LABELSETS)\n",
    "        ax.set_title(arch)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"Score (95% CI)\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.figtext(0.5, 0.01, \"Label Set\", ha=\"center\", fontsize=11)\n",
    "plt.suptitle(\"Micro Precision / Recall / F1 across Label Sets\", fontsize=13)\n",
    "plt.savefig(f\"{out_dir}/barplot_combined_side_by_side.png\", dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
